{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, RNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Data Loading, normalization, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reshape_to_input(X, T, Mat_dim):\n",
    "    \"\"\"\n",
    "    reshapes data\n",
    "    \"\"\"\n",
    "   \n",
    "    [Num_examples, dim] = X.shape\n",
    "    input_X = X[:,2:].reshape(Num_examples, Mat_dim, T)\n",
    "    \n",
    "    return input_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeTowardsMean(X):\n",
    "   # print(X.shape)\n",
    "    \"# Preprocessing: Subtract the mean feature - a question if it is feasible - all features are already in the same scale...\\n\",\n",
    "    # TODO: try -1 1 normalization\n",
    "    mean_feat = np.mean(X, axis=0, keepdims=True)\n",
    "   # print(mean_feat.shape)\n",
    "    X -= mean_feat\n",
    "    # Preprocessing: Divide by standard deviation. This ensures that each feature\\n\",\n",
    "    # has roughly the same scale.\\n\",\n",
    "    std_feat = np.std(X, axis=0, keepdims=True)\n",
    "    X /= std_feat\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_test = np.array([])\n",
    "    #print(X_train.shape)\n",
    "    for Person in range(0,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "        Persons_test = np.append(Persons_test, label, axis = 0)\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    returned_train = np.unique(Persons_train)\n",
    "    print('Persons for test: ')\n",
    "    print(np.unique(Persons_test))\n",
    "    return X_train, Y_train, X_test, Y_test, returned_train\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_val_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    X_val = np.array([])\n",
    "    Y_val = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_val = np.array([])\n",
    "    Persons_test = np.array([])\n",
    "    \n",
    "    train_N =int(np.ceil(N*2/3))\n",
    "    \n",
    "    for Person in range(0,train_N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    \n",
    "     \n",
    "    for Person in range(train_N,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_val= np.append(X_val,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_val = np.append(Y_val, X_all[[indexes], 0])\n",
    "        Persons_val = np.append(Persons_val, label, axis = 0)\n",
    "    X_val=X_val.reshape(Y_val.size, num_dim)  \n",
    "    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "        Persons_test = np.append(Persons_test, label, axis = 0)\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    returned_train = np.unique(Persons_train)\n",
    "    print('Persons for val: ')\n",
    "    print(np.unique(Persons_val))\n",
    "    returned_val = np.unique(Persons_val)\n",
    "    print('Persons for test: ')\n",
    "    print(np.unique(Persons_test))\n",
    "    return X_train, Y_train, X_test, Y_test, X_val, Y_val, returned_train, returned_val\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(inputY, C):\n",
    "    N= inputY.size\n",
    "    Y=np.zeros((N,C))\n",
    "    for i in range (0, inputY.size):\n",
    "        Y[i, int(inputY[i]-1)] = 1\n",
    "        \n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units = 128 \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = LSTM(units, return_sequences=True)( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(units, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my dataset and parameters from a file\n",
    "from data_utils import loadfromfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 SHORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units =  192\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = LSTM(units, return_sequences=False)( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    #X = LSTM(units, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 LONG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units =  192\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = LSTM(units, return_sequences=True)( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    X = LSTM(units, return_sequences=True)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(units, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN Loop through all the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters needed\n",
    "trials = 7\n",
    "epochs = 35\n",
    "batch_size = 15\n",
    "units = 128 # actually defind inside the funciton of the model - TODO - make it here\n",
    "path ='data/3_angles'\n",
    "Files, T, max_dim = loadfromfolder()\n",
    "#Where Files are names, T - my division into frames and max_dim - feature dimensionality\n",
    "fRes = open(\"Results.txt\",\"a+\") \n",
    "fRes.write(\"Number of units in LSTM = %d,  Epochs =  %d and batch size =  %d \\r\\n\" % (units, epochs,batch_size))\n",
    "for file in range(0, len(Files)):\n",
    "    name = Files[file]\n",
    "    t = T[file]\n",
    "    dim = max_dim[file]\n",
    "    name_w_path = path +\"/\"+name\n",
    "    fRes.write(\"Experiment %d from file %s \\r\\n\" % (file,name))\n",
    "    # here all the routine in One cell \n",
    "    X_all = pd.read_csv(name_w_path)\n",
    "    \n",
    "    # the routine to run the same test N times, randomly shuffling the data\n",
    "    Accuracy = X_train = np.zeros(shape=(trials))\n",
    "    for i in range(0,trials):\n",
    "        X_train, Y_train, X_test, Y_test, returned_train = Random_Selection_train_test (X_all.values, 14)\n",
    "        #fRes.write(returned_train) # the presons used for training\n",
    "        namePersons = str(i)+'_'+'file'+ name[:-4] +'.txt'\n",
    "        np.savetxt(namePersons, returned_train, delimiter=',', fmt='%d')   # X is an array\n",
    "        X_train = NormalizeTowardsMean(X_train)\n",
    "        X_test = NormalizeTowardsMean(X_test)\n",
    "        Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "        Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "        X_train = Reshape_to_input(X_train, t, dim)\n",
    "        X_test = Reshape_to_input(X_test, t, dim)\n",
    "        model = Gait_model((X_train[0,:,:]))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train_oh, epochs, batch_size, shuffle=True)\n",
    "        loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "        Accuracy[i]=acc\n",
    "        fRes.write(\"Final accuracy per trial %f \\r\\n\" % (acc))\n",
    "        \n",
    "    print(Accuracy)\n",
    "    fRes.write(\"Final mean accuracy is: %f \\r\\n\" % (np.mean(Accuracy)))\n",
    "fRes.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myoptim=Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filepath, save_best_only =True, monitor = 'val_loss', mode ='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [es, mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myoptim=SGD(lr=0.01, decay=1e-8, momentum=0.9, nesterov=True)\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filepath, save_best_only =True, monitor = 'val_loss', mode ='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [es, mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles_cycles_frames_15t_08f_angles_22persons.csv\n",
      "data/best/angles_cycles_frames_15t_08f_angles_22persons.csv\n",
      "Persons for train: \n",
      "[ 1.  2.  6.  7.  9. 10. 11. 15. 16. 19. 22.]\n",
      "Persons for val: \n",
      "[ 4.  8. 18. 20. 21.]\n",
      "Persons for test: \n",
      "[ 3.  5. 12. 13. 14. 17.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khokhlov/virtenv/ml36/lib/python3.5/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280 samples, validate on 124 samples\n",
      "Epoch 1/40\n",
      "280/280 [==============================] - 4s 14ms/step - loss: 1.0917 - acc: 0.3500 - val_loss: 1.0866 - val_acc: 0.4919\n",
      "Epoch 2/40\n",
      "280/280 [==============================] - 0s 1ms/step - loss: 1.0674 - acc: 0.5179 - val_loss: 1.0630 - val_acc: 0.5645\n",
      "Epoch 3/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 1.0385 - acc: 0.5643 - val_loss: 1.0398 - val_acc: 0.5645\n",
      "Epoch 4/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 1.0091 - acc: 0.5571 - val_loss: 1.0140 - val_acc: 0.5806\n",
      "Epoch 5/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.9667 - acc: 0.6357 - val_loss: 0.9851 - val_acc: 0.5565\n",
      "Epoch 6/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.9299 - acc: 0.6286 - val_loss: 0.9508 - val_acc: 0.5887\n",
      "Epoch 7/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.8771 - acc: 0.7036 - val_loss: 0.9148 - val_acc: 0.5968\n",
      "Epoch 8/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.8205 - acc: 0.6964 - val_loss: 0.8772 - val_acc: 0.6613\n",
      "Epoch 9/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.7756 - acc: 0.7036 - val_loss: 0.8514 - val_acc: 0.6774\n",
      "Epoch 10/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.7187 - acc: 0.7321 - val_loss: 0.8424 - val_acc: 0.6694\n",
      "Epoch 11/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.6800 - acc: 0.7214 - val_loss: 0.8317 - val_acc: 0.6694\n",
      "Epoch 12/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.6438 - acc: 0.7536 - val_loss: 0.8355 - val_acc: 0.6613\n",
      "Epoch 13/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.6254 - acc: 0.7571 - val_loss: 0.8482 - val_acc: 0.6613\n",
      "Epoch 14/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.5846 - acc: 0.8000 - val_loss: 0.8651 - val_acc: 0.6694\n",
      "Epoch 15/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.5567 - acc: 0.8071 - val_loss: 0.8788 - val_acc: 0.6452\n",
      "Epoch 16/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.5382 - acc: 0.7964 - val_loss: 0.8936 - val_acc: 0.6532\n",
      "Epoch 17/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.5362 - acc: 0.8071 - val_loss: 0.9166 - val_acc: 0.6613\n",
      "Epoch 18/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.4988 - acc: 0.8286 - val_loss: 0.9255 - val_acc: 0.6452\n",
      "Epoch 19/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.5011 - acc: 0.8250 - val_loss: 0.9199 - val_acc: 0.6452\n",
      "Epoch 20/40\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.4718 - acc: 0.8500 - val_loss: 0.9524 - val_acc: 0.6532\n",
      "Epoch 21/40\n",
      "280/280 [==============================] - 0s 1ms/step - loss: 0.4658 - acc: 0.8321 - val_loss: 0.9659 - val_acc: 0.6371\n",
      "Epoch 22/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.4403 - acc: 0.8500 - val_loss: 0.9783 - val_acc: 0.6371\n",
      "Epoch 23/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.4290 - acc: 0.8500 - val_loss: 0.9902 - val_acc: 0.6452\n",
      "Epoch 24/40\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.8464 - val_loss: 1.0080 - val_acc: 0.6452\n",
      "Epoch 25/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.3918 - acc: 0.8679 - val_loss: 1.0289 - val_acc: 0.6613\n",
      "Epoch 26/40\n",
      "280/280 [==============================] - 0s 2ms/step - loss: 0.3911 - acc: 0.8714 - val_loss: 1.0510 - val_acc: 0.6371\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "143/143 [==============================] - 0s 292us/step\n",
      "Persons for train: \n",
      "[ 6.  9. 10. 11. 13. 14. 17. 18. 19. 20. 22.]\n",
      "Persons for val: \n",
      "[ 1.  2.  7.  8. 21.]\n",
      "Persons for test: \n",
      "[ 3.  4.  5. 12. 15. 16.]\n",
      "Train on 284 samples, validate on 121 samples\n",
      "Epoch 1/40\n",
      "284/284 [==============================] - 3s 10ms/step - loss: 1.0926 - acc: 0.3908 - val_loss: 1.0883 - val_acc: 0.4628\n",
      "Epoch 2/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 1.0806 - acc: 0.4542 - val_loss: 1.0784 - val_acc: 0.5950\n",
      "Epoch 3/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 1.0670 - acc: 0.5000 - val_loss: 1.0698 - val_acc: 0.6033\n",
      "Epoch 4/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 1.0667 - acc: 0.5035 - val_loss: 1.0617 - val_acc: 0.6116\n",
      "Epoch 5/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 1.0507 - acc: 0.5599 - val_loss: 1.0543 - val_acc: 0.6116\n",
      "Epoch 6/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.0382 - acc: 0.6549 - val_loss: 1.0475 - val_acc: 0.6198\n",
      "Epoch 7/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.0312 - acc: 0.6338 - val_loss: 1.0410 - val_acc: 0.6198\n",
      "Epoch 8/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.0257 - acc: 0.6549 - val_loss: 1.0345 - val_acc: 0.6198\n",
      "Epoch 9/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.0208 - acc: 0.6268 - val_loss: 1.0282 - val_acc: 0.6198\n",
      "Epoch 10/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 1.0102 - acc: 0.6866 - val_loss: 1.0219 - val_acc: 0.6198\n",
      "Epoch 11/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9954 - acc: 0.6761 - val_loss: 1.0151 - val_acc: 0.6364\n",
      "Epoch 12/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9897 - acc: 0.6655 - val_loss: 1.0088 - val_acc: 0.6364\n",
      "Epoch 13/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9848 - acc: 0.6514 - val_loss: 1.0025 - val_acc: 0.6364\n",
      "Epoch 14/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.9803 - acc: 0.6761 - val_loss: 0.9964 - val_acc: 0.6364\n",
      "Epoch 15/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9750 - acc: 0.6796 - val_loss: 0.9906 - val_acc: 0.6364\n",
      "Epoch 16/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9581 - acc: 0.6725 - val_loss: 0.9844 - val_acc: 0.6446\n",
      "Epoch 17/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.9561 - acc: 0.6901 - val_loss: 0.9781 - val_acc: 0.6446\n",
      "Epoch 18/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.9355 - acc: 0.6901 - val_loss: 0.9720 - val_acc: 0.6446\n",
      "Epoch 19/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9317 - acc: 0.6690 - val_loss: 0.9659 - val_acc: 0.6446\n",
      "Epoch 20/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.9284 - acc: 0.6373 - val_loss: 0.9597 - val_acc: 0.6364\n",
      "Epoch 21/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.9153 - acc: 0.7289 - val_loss: 0.9536 - val_acc: 0.6446\n",
      "Epoch 22/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9095 - acc: 0.6725 - val_loss: 0.9473 - val_acc: 0.6446\n",
      "Epoch 23/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.9053 - acc: 0.7007 - val_loss: 0.9410 - val_acc: 0.6446\n",
      "Epoch 24/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8965 - acc: 0.6937 - val_loss: 0.9354 - val_acc: 0.6446\n",
      "Epoch 25/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8875 - acc: 0.7042 - val_loss: 0.9304 - val_acc: 0.6446\n",
      "Epoch 26/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8873 - acc: 0.6796 - val_loss: 0.9250 - val_acc: 0.6446\n",
      "Epoch 27/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.8743 - acc: 0.6585 - val_loss: 0.9196 - val_acc: 0.6446\n",
      "Epoch 28/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8614 - acc: 0.6725 - val_loss: 0.9144 - val_acc: 0.6446\n",
      "Epoch 29/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8525 - acc: 0.6937 - val_loss: 0.9092 - val_acc: 0.6446\n",
      "Epoch 30/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.8483 - acc: 0.6796 - val_loss: 0.9039 - val_acc: 0.6364\n",
      "Epoch 31/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8314 - acc: 0.6937 - val_loss: 0.8991 - val_acc: 0.6446\n",
      "Epoch 32/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.8355 - acc: 0.6901 - val_loss: 0.8952 - val_acc: 0.6364\n",
      "Epoch 33/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.8248 - acc: 0.6901 - val_loss: 0.8919 - val_acc: 0.6364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.8155 - acc: 0.6796 - val_loss: 0.8879 - val_acc: 0.6364\n",
      "Epoch 35/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.8083 - acc: 0.6831 - val_loss: 0.8842 - val_acc: 0.6364\n",
      "Epoch 36/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.8004 - acc: 0.7007 - val_loss: 0.8812 - val_acc: 0.6364\n",
      "Epoch 37/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.7967 - acc: 0.7183 - val_loss: 0.8782 - val_acc: 0.6446\n",
      "Epoch 38/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.7872 - acc: 0.6901 - val_loss: 0.8756 - val_acc: 0.6446\n",
      "Epoch 39/40\n",
      "284/284 [==============================] - 0s 1ms/step - loss: 0.7879 - acc: 0.7042 - val_loss: 0.8730 - val_acc: 0.6446\n",
      "Epoch 40/40\n",
      "284/284 [==============================] - 0s 2ms/step - loss: 0.7851 - acc: 0.7183 - val_loss: 0.8715 - val_acc: 0.6446\n",
      "142/142 [==============================] - 0s 410us/step\n",
      "Persons for train: \n",
      "[ 1.  2.  7.  9. 11. 14. 15. 16. 19. 21. 22.]\n",
      "Persons for val: \n",
      "[ 4.  5.  8. 10. 12.]\n",
      "Persons for test: \n",
      "[ 3.  6. 13. 17. 18. 20.]\n",
      "Train on 282 samples, validate on 129 samples\n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.0832 - acc: 0.4504 - val_loss: 1.0810 - val_acc: 0.5504\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 1.0702 - acc: 0.5248 - val_loss: 1.0688 - val_acc: 0.5271\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 1.0541 - acc: 0.5887 - val_loss: 1.0581 - val_acc: 0.5039\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 1.0394 - acc: 0.6206 - val_loss: 1.0490 - val_acc: 0.5039\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 1.0203 - acc: 0.6560 - val_loss: 1.0407 - val_acc: 0.4961\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 1.0168 - acc: 0.6383 - val_loss: 1.0332 - val_acc: 0.4729\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 1.0038 - acc: 0.6454 - val_loss: 1.0264 - val_acc: 0.4806\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.9967 - acc: 0.6312 - val_loss: 1.0194 - val_acc: 0.4884\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.9775 - acc: 0.6277 - val_loss: 1.0130 - val_acc: 0.4806\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.9744 - acc: 0.6418 - val_loss: 1.0067 - val_acc: 0.4961\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.9614 - acc: 0.6312 - val_loss: 1.0007 - val_acc: 0.5039\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.9521 - acc: 0.6809 - val_loss: 0.9945 - val_acc: 0.5039\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.9499 - acc: 0.6525 - val_loss: 0.9887 - val_acc: 0.5039\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.9418 - acc: 0.6631 - val_loss: 0.9835 - val_acc: 0.5039\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.9232 - acc: 0.6773 - val_loss: 0.9786 - val_acc: 0.5039\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.9203 - acc: 0.6738 - val_loss: 0.9731 - val_acc: 0.5039\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.9091 - acc: 0.6738 - val_loss: 0.9678 - val_acc: 0.5116\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.8995 - acc: 0.6667 - val_loss: 0.9626 - val_acc: 0.5349\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.8912 - acc: 0.7092 - val_loss: 0.9578 - val_acc: 0.5349\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.8872 - acc: 0.6702 - val_loss: 0.9530 - val_acc: 0.5349\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.8736 - acc: 0.7057 - val_loss: 0.9482 - val_acc: 0.5349\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8747 - acc: 0.6525 - val_loss: 0.9435 - val_acc: 0.5349\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.8562 - acc: 0.7128 - val_loss: 0.9398 - val_acc: 0.5426\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8500 - acc: 0.7092 - val_loss: 0.9355 - val_acc: 0.5426\n",
      "Epoch 25/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8498 - acc: 0.7128 - val_loss: 0.9317 - val_acc: 0.5426\n",
      "Epoch 26/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8277 - acc: 0.7057 - val_loss: 0.9277 - val_acc: 0.5426\n",
      "Epoch 27/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8309 - acc: 0.7163 - val_loss: 0.9236 - val_acc: 0.5426\n",
      "Epoch 28/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8231 - acc: 0.7199 - val_loss: 0.9201 - val_acc: 0.5426\n",
      "Epoch 29/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8097 - acc: 0.7305 - val_loss: 0.9175 - val_acc: 0.5504\n",
      "Epoch 30/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.8029 - acc: 0.7376 - val_loss: 0.9145 - val_acc: 0.5426\n",
      "Epoch 31/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7920 - acc: 0.7234 - val_loss: 0.9115 - val_acc: 0.5426\n",
      "Epoch 32/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7889 - acc: 0.7163 - val_loss: 0.9084 - val_acc: 0.5504\n",
      "Epoch 33/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7791 - acc: 0.7305 - val_loss: 0.9072 - val_acc: 0.5504\n",
      "Epoch 34/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7751 - acc: 0.7057 - val_loss: 0.9052 - val_acc: 0.5581\n",
      "Epoch 35/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7635 - acc: 0.7270 - val_loss: 0.9036 - val_acc: 0.5659\n",
      "Epoch 36/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7665 - acc: 0.7163 - val_loss: 0.9025 - val_acc: 0.5659\n",
      "Epoch 37/40\n",
      "282/282 [==============================] - 0s 1ms/step - loss: 0.7631 - acc: 0.7234 - val_loss: 0.9018 - val_acc: 0.5659\n",
      "Epoch 38/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7497 - acc: 0.7447 - val_loss: 0.8998 - val_acc: 0.5659\n",
      "Epoch 39/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7409 - acc: 0.7270 - val_loss: 0.8986 - val_acc: 0.5659\n",
      "Epoch 40/40\n",
      "282/282 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.7447 - val_loss: 0.8974 - val_acc: 0.5581\n",
      "136/136 [==============================] - 0s 485us/step\n",
      "Persons for train: \n",
      "[ 1.  5.  6.  9. 10. 14. 15. 16. 17. 19. 20.]\n",
      "Persons for val: \n",
      "[ 3.  7.  8. 11. 13.]\n",
      "Persons for test: \n",
      "[ 2.  4. 12. 18. 21. 22.]\n",
      "Train on 265 samples, validate on 110 samples\n",
      "Epoch 1/40\n",
      "265/265 [==============================] - 2s 8ms/step - loss: 1.0945 - acc: 0.3962 - val_loss: 1.0931 - val_acc: 0.4182\n",
      "Epoch 2/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 1.0799 - acc: 0.4528 - val_loss: 1.0807 - val_acc: 0.5545\n",
      "Epoch 3/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 1.0649 - acc: 0.5019 - val_loss: 1.0685 - val_acc: 0.5364\n",
      "Epoch 4/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 1.0453 - acc: 0.5849 - val_loss: 1.0579 - val_acc: 0.5636\n",
      "Epoch 5/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 1.0327 - acc: 0.6340 - val_loss: 1.0477 - val_acc: 0.5636\n",
      "Epoch 6/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 1.0282 - acc: 0.5849 - val_loss: 1.0378 - val_acc: 0.5727\n",
      "Epoch 7/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 1.0138 - acc: 0.6566 - val_loss: 1.0284 - val_acc: 0.5727\n",
      "Epoch 8/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.9964 - acc: 0.6566 - val_loss: 1.0189 - val_acc: 0.5636\n",
      "Epoch 9/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.9853 - acc: 0.6340 - val_loss: 1.0094 - val_acc: 0.5727\n",
      "Epoch 10/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.9728 - acc: 0.6679 - val_loss: 1.0003 - val_acc: 0.5727\n",
      "Epoch 11/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.9634 - acc: 0.6642 - val_loss: 0.9905 - val_acc: 0.5909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.9463 - acc: 0.6755 - val_loss: 0.9807 - val_acc: 0.5909\n",
      "Epoch 13/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.9344 - acc: 0.7094 - val_loss: 0.9708 - val_acc: 0.6000\n",
      "Epoch 14/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.9244 - acc: 0.6679 - val_loss: 0.9605 - val_acc: 0.6000\n",
      "Epoch 15/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.9122 - acc: 0.7019 - val_loss: 0.9507 - val_acc: 0.6091\n",
      "Epoch 16/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.9028 - acc: 0.6755 - val_loss: 0.9397 - val_acc: 0.6182\n",
      "Epoch 17/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.8930 - acc: 0.6906 - val_loss: 0.9290 - val_acc: 0.6273\n",
      "Epoch 18/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.8718 - acc: 0.6868 - val_loss: 0.9188 - val_acc: 0.6273\n",
      "Epoch 19/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.8556 - acc: 0.7057 - val_loss: 0.9084 - val_acc: 0.6545\n",
      "Epoch 20/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.8489 - acc: 0.7132 - val_loss: 0.8977 - val_acc: 0.6455\n",
      "Epoch 21/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.8308 - acc: 0.6943 - val_loss: 0.8874 - val_acc: 0.6364\n",
      "Epoch 22/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.8290 - acc: 0.6981 - val_loss: 0.8773 - val_acc: 0.6364\n",
      "Epoch 23/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.8103 - acc: 0.6981 - val_loss: 0.8674 - val_acc: 0.6455\n",
      "Epoch 24/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7990 - acc: 0.7132 - val_loss: 0.8572 - val_acc: 0.6455\n",
      "Epoch 25/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.7818 - acc: 0.7132 - val_loss: 0.8481 - val_acc: 0.6455\n",
      "Epoch 26/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7733 - acc: 0.7208 - val_loss: 0.8376 - val_acc: 0.6455\n",
      "Epoch 27/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7642 - acc: 0.7057 - val_loss: 0.8283 - val_acc: 0.6545\n",
      "Epoch 28/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7601 - acc: 0.7170 - val_loss: 0.8195 - val_acc: 0.6545\n",
      "Epoch 29/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7371 - acc: 0.7132 - val_loss: 0.8103 - val_acc: 0.6545\n",
      "Epoch 30/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7315 - acc: 0.7321 - val_loss: 0.8025 - val_acc: 0.6545\n",
      "Epoch 31/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7128 - acc: 0.7472 - val_loss: 0.7938 - val_acc: 0.6545\n",
      "Epoch 32/40\n",
      "265/265 [==============================] - 0s 1ms/step - loss: 0.7081 - acc: 0.7434 - val_loss: 0.7853 - val_acc: 0.6545\n",
      "Epoch 33/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.7080 - acc: 0.7321 - val_loss: 0.7785 - val_acc: 0.6545\n",
      "Epoch 34/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6990 - acc: 0.7358 - val_loss: 0.7716 - val_acc: 0.6545\n",
      "Epoch 35/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6788 - acc: 0.7585 - val_loss: 0.7618 - val_acc: 0.6636\n",
      "Epoch 36/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6814 - acc: 0.7358 - val_loss: 0.7543 - val_acc: 0.6727\n",
      "Epoch 37/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6692 - acc: 0.7396 - val_loss: 0.7474 - val_acc: 0.6727\n",
      "Epoch 38/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6580 - acc: 0.7396 - val_loss: 0.7410 - val_acc: 0.6727\n",
      "Epoch 39/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6567 - acc: 0.7396 - val_loss: 0.7358 - val_acc: 0.6727\n",
      "Epoch 40/40\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 0.6472 - acc: 0.7396 - val_loss: 0.7297 - val_acc: 0.6818\n",
      "172/172 [==============================] - 0s 434us/step\n",
      "Persons for train: \n",
      "[ 2.  6.  7. 10. 11. 12. 14. 16. 17. 21. 22.]\n",
      "Persons for val: \n",
      "[ 1.  3. 15. 18. 19.]\n",
      "Persons for test: \n",
      "[ 4.  5.  8.  9. 13. 20.]\n",
      "Train on 286 samples, validate on 105 samples\n",
      "Epoch 1/40\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 1.0714 - acc: 0.4755 - val_loss: 1.0734 - val_acc: 0.4952\n",
      "Epoch 2/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 1.0505 - acc: 0.5664 - val_loss: 1.0569 - val_acc: 0.5333\n",
      "Epoch 3/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 1.0287 - acc: 0.5909 - val_loss: 1.0410 - val_acc: 0.5619\n",
      "Epoch 4/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 1.0105 - acc: 0.6119 - val_loss: 1.0265 - val_acc: 0.6000\n",
      "Epoch 5/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9964 - acc: 0.5699 - val_loss: 1.0133 - val_acc: 0.6000\n",
      "Epoch 6/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9815 - acc: 0.5979 - val_loss: 1.0010 - val_acc: 0.6000\n",
      "Epoch 7/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9616 - acc: 0.6189 - val_loss: 0.9900 - val_acc: 0.6000\n",
      "Epoch 8/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9502 - acc: 0.5769 - val_loss: 0.9799 - val_acc: 0.6095\n",
      "Epoch 9/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9342 - acc: 0.5874 - val_loss: 0.9698 - val_acc: 0.6095\n",
      "Epoch 10/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9179 - acc: 0.5979 - val_loss: 0.9610 - val_acc: 0.6095\n",
      "Epoch 11/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.9048 - acc: 0.5734 - val_loss: 0.9526 - val_acc: 0.5905\n",
      "Epoch 12/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.8899 - acc: 0.6294 - val_loss: 0.9447 - val_acc: 0.5905\n",
      "Epoch 13/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8745 - acc: 0.6154 - val_loss: 0.9378 - val_acc: 0.5905\n",
      "Epoch 14/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8642 - acc: 0.6469 - val_loss: 0.9315 - val_acc: 0.5905\n",
      "Epoch 15/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8575 - acc: 0.6294 - val_loss: 0.9265 - val_acc: 0.6095\n",
      "Epoch 16/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8472 - acc: 0.6364 - val_loss: 0.9220 - val_acc: 0.6000\n",
      "Epoch 17/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8342 - acc: 0.6713 - val_loss: 0.9187 - val_acc: 0.6000\n",
      "Epoch 18/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.8365 - acc: 0.6399 - val_loss: 0.9160 - val_acc: 0.5810\n",
      "Epoch 19/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8408 - acc: 0.6084 - val_loss: 0.9134 - val_acc: 0.5905\n",
      "Epoch 20/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8190 - acc: 0.6364 - val_loss: 0.9113 - val_acc: 0.6000\n",
      "Epoch 21/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8174 - acc: 0.6294 - val_loss: 0.9097 - val_acc: 0.6000\n",
      "Epoch 22/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8051 - acc: 0.6399 - val_loss: 0.9082 - val_acc: 0.6000\n",
      "Epoch 23/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.8096 - acc: 0.6119 - val_loss: 0.9073 - val_acc: 0.5714\n",
      "Epoch 24/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7898 - acc: 0.6678 - val_loss: 0.9061 - val_acc: 0.5810\n",
      "Epoch 25/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7991 - acc: 0.6573 - val_loss: 0.9059 - val_acc: 0.5714\n",
      "Epoch 26/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7913 - acc: 0.6259 - val_loss: 0.9061 - val_acc: 0.5714\n",
      "Epoch 27/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7860 - acc: 0.6399 - val_loss: 0.9055 - val_acc: 0.5619\n",
      "Epoch 28/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7871 - acc: 0.6434 - val_loss: 0.9053 - val_acc: 0.5714\n",
      "Epoch 29/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7893 - acc: 0.6189 - val_loss: 0.9050 - val_acc: 0.5714\n",
      "Epoch 30/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7791 - acc: 0.6154 - val_loss: 0.9051 - val_acc: 0.5810\n",
      "Epoch 31/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7776 - acc: 0.6678 - val_loss: 0.9051 - val_acc: 0.5810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7815 - acc: 0.6469 - val_loss: 0.9047 - val_acc: 0.5810\n",
      "Epoch 33/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7701 - acc: 0.6538 - val_loss: 0.9048 - val_acc: 0.5810\n",
      "Epoch 34/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7723 - acc: 0.6713 - val_loss: 0.9049 - val_acc: 0.5810\n",
      "Epoch 35/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7726 - acc: 0.6818 - val_loss: 0.9052 - val_acc: 0.5714\n",
      "Epoch 36/40\n",
      "286/286 [==============================] - 0s 2ms/step - loss: 0.7661 - acc: 0.6503 - val_loss: 0.9054 - val_acc: 0.5714\n",
      "Epoch 37/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7600 - acc: 0.6573 - val_loss: 0.9062 - val_acc: 0.5714\n",
      "Epoch 38/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7595 - acc: 0.6608 - val_loss: 0.9068 - val_acc: 0.5714\n",
      "Epoch 39/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7608 - acc: 0.6713 - val_loss: 0.9074 - val_acc: 0.5714\n",
      "Epoch 40/40\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.7514 - acc: 0.6713 - val_loss: 0.9073 - val_acc: 0.5714\n",
      "156/156 [==============================] - 0s 409us/step\n",
      "Persons for train: \n",
      "[ 1.  3.  6.  7.  8. 10. 11. 16. 18. 20. 22.]\n",
      "Persons for val: \n",
      "[ 2. 13. 14. 19. 21.]\n",
      "Persons for test: \n",
      "[ 4.  5.  9. 12. 15. 17.]\n",
      "Train on 256 samples, validate on 120 samples\n",
      "Epoch 1/40\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 1.0844 - acc: 0.4492 - val_loss: 1.0729 - val_acc: 0.5167\n",
      "Epoch 2/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0775 - acc: 0.4805 - val_loss: 1.0581 - val_acc: 0.5417\n",
      "Epoch 3/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0537 - acc: 0.5547 - val_loss: 1.0439 - val_acc: 0.6250\n",
      "Epoch 4/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0385 - acc: 0.5664 - val_loss: 1.0301 - val_acc: 0.6417\n",
      "Epoch 5/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0357 - acc: 0.5742 - val_loss: 1.0175 - val_acc: 0.6500\n",
      "Epoch 6/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0174 - acc: 0.6055 - val_loss: 1.0054 - val_acc: 0.6500\n",
      "Epoch 7/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 1.0103 - acc: 0.5977 - val_loss: 0.9936 - val_acc: 0.6500\n",
      "Epoch 8/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9982 - acc: 0.5820 - val_loss: 0.9817 - val_acc: 0.6667\n",
      "Epoch 9/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9863 - acc: 0.6016 - val_loss: 0.9706 - val_acc: 0.6667\n",
      "Epoch 10/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9750 - acc: 0.6523 - val_loss: 0.9589 - val_acc: 0.6500\n",
      "Epoch 11/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9589 - acc: 0.6563 - val_loss: 0.9475 - val_acc: 0.6417\n",
      "Epoch 12/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9521 - acc: 0.6367 - val_loss: 0.9358 - val_acc: 0.6333\n",
      "Epoch 13/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9426 - acc: 0.6484 - val_loss: 0.9242 - val_acc: 0.6333\n",
      "Epoch 14/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9270 - acc: 0.6484 - val_loss: 0.9138 - val_acc: 0.6417\n",
      "Epoch 15/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9205 - acc: 0.6641 - val_loss: 0.9035 - val_acc: 0.6500\n",
      "Epoch 16/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.9050 - acc: 0.6602 - val_loss: 0.8929 - val_acc: 0.6500\n",
      "Epoch 17/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8903 - acc: 0.6719 - val_loss: 0.8828 - val_acc: 0.6583\n",
      "Epoch 18/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8861 - acc: 0.6484 - val_loss: 0.8731 - val_acc: 0.6667\n",
      "Epoch 19/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8808 - acc: 0.6875 - val_loss: 0.8645 - val_acc: 0.6667\n",
      "Epoch 20/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8715 - acc: 0.6563 - val_loss: 0.8563 - val_acc: 0.6667\n",
      "Epoch 21/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8575 - acc: 0.6641 - val_loss: 0.8488 - val_acc: 0.6583\n",
      "Epoch 22/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8579 - acc: 0.6758 - val_loss: 0.8407 - val_acc: 0.6583\n",
      "Epoch 23/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8512 - acc: 0.6680 - val_loss: 0.8340 - val_acc: 0.6583\n",
      "Epoch 24/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8394 - acc: 0.6719 - val_loss: 0.8290 - val_acc: 0.6583\n",
      "Epoch 25/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.8334 - acc: 0.6797 - val_loss: 0.8234 - val_acc: 0.6583\n",
      "Epoch 26/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.8257 - acc: 0.6836 - val_loss: 0.8187 - val_acc: 0.6583\n",
      "Epoch 27/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.8248 - acc: 0.6680 - val_loss: 0.8136 - val_acc: 0.6583\n",
      "Epoch 28/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.8116 - acc: 0.6758 - val_loss: 0.8089 - val_acc: 0.6667\n",
      "Epoch 29/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.8094 - acc: 0.6641 - val_loss: 0.8035 - val_acc: 0.6667\n",
      "Epoch 30/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.7967 - acc: 0.6758 - val_loss: 0.7976 - val_acc: 0.6667\n",
      "Epoch 31/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.8136 - acc: 0.6641 - val_loss: 0.7935 - val_acc: 0.6667\n",
      "Epoch 32/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.7978 - acc: 0.6641 - val_loss: 0.7890 - val_acc: 0.6667\n",
      "Epoch 33/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7930 - acc: 0.6680 - val_loss: 0.7851 - val_acc: 0.6667\n",
      "Epoch 34/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7894 - acc: 0.6914 - val_loss: 0.7803 - val_acc: 0.6667\n",
      "Epoch 35/40\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 0.7799 - acc: 0.7070 - val_loss: 0.7769 - val_acc: 0.6667\n",
      "Epoch 36/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7810 - acc: 0.6758 - val_loss: 0.7731 - val_acc: 0.6667\n",
      "Epoch 37/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7637 - acc: 0.6953 - val_loss: 0.7696 - val_acc: 0.6667\n",
      "Epoch 38/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7799 - acc: 0.6797 - val_loss: 0.7672 - val_acc: 0.6667\n",
      "Epoch 39/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7704 - acc: 0.6914 - val_loss: 0.7632 - val_acc: 0.6667\n",
      "Epoch 40/40\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 0.7590 - acc: 0.6992 - val_loss: 0.7596 - val_acc: 0.6667\n",
      "171/171 [==============================] - 0s 415us/step\n",
      "Persons for train: \n",
      "[ 4.  6.  8. 10. 13. 14. 16. 17. 18. 19. 20.]\n",
      "Persons for val: \n",
      "[ 2. 11. 12. 15. 22.]\n",
      "Persons for test: \n",
      "[ 1.  3.  5.  7.  9. 21.]\n",
      "Train on 258 samples, validate on 134 samples\n",
      "Epoch 1/40\n",
      "258/258 [==============================] - 3s 13ms/step - loss: 1.0930 - acc: 0.3643 - val_loss: 1.0798 - val_acc: 0.6045\n",
      "Epoch 2/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 1.0699 - acc: 0.4806 - val_loss: 1.0659 - val_acc: 0.5746\n",
      "Epoch 3/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 1.0499 - acc: 0.5930 - val_loss: 1.0531 - val_acc: 0.5522\n",
      "Epoch 4/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 1.0301 - acc: 0.6512 - val_loss: 1.0414 - val_acc: 0.5448\n",
      "Epoch 5/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 1.0146 - acc: 0.6473 - val_loss: 1.0317 - val_acc: 0.5448\n",
      "Epoch 6/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9989 - acc: 0.6357 - val_loss: 1.0213 - val_acc: 0.5373\n",
      "Epoch 7/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9884 - acc: 0.6357 - val_loss: 1.0111 - val_acc: 0.5149\n",
      "Epoch 8/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9666 - acc: 0.6357 - val_loss: 1.0018 - val_acc: 0.5149\n",
      "Epoch 9/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9582 - acc: 0.6860 - val_loss: 0.9928 - val_acc: 0.5075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9431 - acc: 0.6744 - val_loss: 0.9840 - val_acc: 0.5000\n",
      "Epoch 11/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9207 - acc: 0.6822 - val_loss: 0.9749 - val_acc: 0.5000\n",
      "Epoch 12/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.9118 - acc: 0.6783 - val_loss: 0.9664 - val_acc: 0.5000\n",
      "Epoch 13/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8987 - acc: 0.6938 - val_loss: 0.9584 - val_acc: 0.5000\n",
      "Epoch 14/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8868 - acc: 0.6550 - val_loss: 0.9513 - val_acc: 0.5149\n",
      "Epoch 15/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8703 - acc: 0.6938 - val_loss: 0.9452 - val_acc: 0.5000\n",
      "Epoch 16/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8597 - acc: 0.7093 - val_loss: 0.9400 - val_acc: 0.5149\n",
      "Epoch 17/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8529 - acc: 0.6667 - val_loss: 0.9358 - val_acc: 0.5149\n",
      "Epoch 18/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8394 - acc: 0.6279 - val_loss: 0.9316 - val_acc: 0.5149\n",
      "Epoch 19/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8258 - acc: 0.6589 - val_loss: 0.9282 - val_acc: 0.5075\n",
      "Epoch 20/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8231 - acc: 0.6860 - val_loss: 0.9257 - val_acc: 0.5075\n",
      "Epoch 21/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.8003 - acc: 0.6977 - val_loss: 0.9239 - val_acc: 0.5075\n",
      "Epoch 22/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7951 - acc: 0.7016 - val_loss: 0.9225 - val_acc: 0.5075\n",
      "Epoch 23/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7909 - acc: 0.6822 - val_loss: 0.9224 - val_acc: 0.5149\n",
      "Epoch 24/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7896 - acc: 0.6938 - val_loss: 0.9223 - val_acc: 0.5224\n",
      "Epoch 25/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7744 - acc: 0.6822 - val_loss: 0.9222 - val_acc: 0.5224\n",
      "Epoch 26/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7551 - acc: 0.7248 - val_loss: 0.9226 - val_acc: 0.5373\n",
      "Epoch 27/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7689 - acc: 0.6899 - val_loss: 0.9238 - val_acc: 0.5299\n",
      "Epoch 28/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7434 - acc: 0.7016 - val_loss: 0.9248 - val_acc: 0.5299\n",
      "Epoch 29/40\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.7407 - acc: 0.7093 - val_loss: 0.9267 - val_acc: 0.5373\n",
      "Epoch 30/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7361 - acc: 0.6860 - val_loss: 0.9281 - val_acc: 0.5373\n",
      "Epoch 31/40\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.7348 - acc: 0.7054 - val_loss: 0.9296 - val_acc: 0.5373\n",
      "Epoch 32/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7261 - acc: 0.7016 - val_loss: 0.9310 - val_acc: 0.5373\n",
      "Epoch 33/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7162 - acc: 0.7132 - val_loss: 0.9323 - val_acc: 0.5448\n",
      "Epoch 34/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7227 - acc: 0.7093 - val_loss: 0.9333 - val_acc: 0.5448\n",
      "Epoch 35/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7213 - acc: 0.7054 - val_loss: 0.9351 - val_acc: 0.5448\n",
      "Epoch 36/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7097 - acc: 0.7132 - val_loss: 0.9372 - val_acc: 0.5373\n",
      "Epoch 37/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7183 - acc: 0.7093 - val_loss: 0.9390 - val_acc: 0.5373\n",
      "Epoch 38/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7119 - acc: 0.7054 - val_loss: 0.9408 - val_acc: 0.5373\n",
      "Epoch 39/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.7013 - acc: 0.7016 - val_loss: 0.9427 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "Epoch 40/40\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.6877 - acc: 0.7287 - val_loss: 0.9428 - val_acc: 0.5373\n",
      "155/155 [==============================] - 0s 445us/step\n",
      "Persons for train: \n",
      "[ 3.  6.  8. 10. 12. 15. 16. 18. 19. 21. 22.]\n",
      "Persons for val: \n",
      "[ 1.  4.  5.  9. 13.]\n",
      "Persons for test: \n",
      "[ 2.  7. 11. 14. 17. 20.]\n",
      "Train on 257 samples, validate on 137 samples\n",
      "Epoch 1/40\n",
      "257/257 [==============================] - 3s 11ms/step - loss: 1.0936 - acc: 0.3502 - val_loss: 1.0938 - val_acc: 0.3869\n",
      "Epoch 2/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0871 - acc: 0.3930 - val_loss: 1.0921 - val_acc: 0.4015\n",
      "Epoch 3/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0913 - acc: 0.3307 - val_loss: 1.0906 - val_acc: 0.4234\n",
      "Epoch 4/40\n",
      "257/257 [==============================] - 0s 1ms/step - loss: 1.0898 - acc: 0.3930 - val_loss: 1.0892 - val_acc: 0.4526\n",
      "Epoch 5/40\n",
      "257/257 [==============================] - 0s 1ms/step - loss: 1.0912 - acc: 0.3696 - val_loss: 1.0879 - val_acc: 0.4672\n",
      "Epoch 6/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0913 - acc: 0.3696 - val_loss: 1.0867 - val_acc: 0.4818\n",
      "Epoch 7/40\n",
      "257/257 [==============================] - 0s 1ms/step - loss: 1.0878 - acc: 0.3502 - val_loss: 1.0856 - val_acc: 0.5109\n",
      "Epoch 8/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0834 - acc: 0.3891 - val_loss: 1.0846 - val_acc: 0.5109\n",
      "Epoch 9/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0900 - acc: 0.4047 - val_loss: 1.0837 - val_acc: 0.5109\n",
      "Epoch 10/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0820 - acc: 0.4514 - val_loss: 1.0828 - val_acc: 0.5182\n",
      "Epoch 11/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0767 - acc: 0.4553 - val_loss: 1.0819 - val_acc: 0.5109\n",
      "Epoch 12/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0795 - acc: 0.4086 - val_loss: 1.0811 - val_acc: 0.5109\n",
      "Epoch 13/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0809 - acc: 0.4163 - val_loss: 1.0803 - val_acc: 0.5182\n",
      "Epoch 14/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0851 - acc: 0.3696 - val_loss: 1.0796 - val_acc: 0.5401\n",
      "Epoch 15/40\n",
      "257/257 [==============================] - 0s 1ms/step - loss: 1.0778 - acc: 0.4708 - val_loss: 1.0788 - val_acc: 0.5474\n",
      "Epoch 16/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0772 - acc: 0.4630 - val_loss: 1.0782 - val_acc: 0.5474\n",
      "Epoch 17/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0744 - acc: 0.4786 - val_loss: 1.0775 - val_acc: 0.5474\n",
      "Epoch 18/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0764 - acc: 0.4553 - val_loss: 1.0768 - val_acc: 0.5474\n",
      "Epoch 19/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0801 - acc: 0.3852 - val_loss: 1.0762 - val_acc: 0.5474\n",
      "Epoch 20/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0748 - acc: 0.4864 - val_loss: 1.0755 - val_acc: 0.5474\n",
      "Epoch 21/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0783 - acc: 0.4397 - val_loss: 1.0749 - val_acc: 0.5547\n",
      "Epoch 22/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0748 - acc: 0.4553 - val_loss: 1.0743 - val_acc: 0.5547\n",
      "Epoch 23/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0702 - acc: 0.4981 - val_loss: 1.0737 - val_acc: 0.5547\n",
      "Epoch 24/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0707 - acc: 0.4825 - val_loss: 1.0731 - val_acc: 0.5620\n",
      "Epoch 25/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0684 - acc: 0.5058 - val_loss: 1.0725 - val_acc: 0.5620\n",
      "Epoch 26/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0720 - acc: 0.4942 - val_loss: 1.0720 - val_acc: 0.5620\n",
      "Epoch 27/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0703 - acc: 0.4514 - val_loss: 1.0714 - val_acc: 0.5620\n",
      "Epoch 28/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0669 - acc: 0.4864 - val_loss: 1.0709 - val_acc: 0.5620\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0726 - acc: 0.4397 - val_loss: 1.0704 - val_acc: 0.5547\n",
      "Epoch 30/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0673 - acc: 0.5019 - val_loss: 1.0699 - val_acc: 0.5547\n",
      "Epoch 31/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0656 - acc: 0.5019 - val_loss: 1.0693 - val_acc: 0.5547\n",
      "Epoch 32/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0661 - acc: 0.4981 - val_loss: 1.0688 - val_acc: 0.5620\n",
      "Epoch 33/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0707 - acc: 0.4786 - val_loss: 1.0683 - val_acc: 0.5693\n",
      "Epoch 34/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0716 - acc: 0.4280 - val_loss: 1.0679 - val_acc: 0.5839\n",
      "Epoch 35/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0639 - acc: 0.5058 - val_loss: 1.0674 - val_acc: 0.5839\n",
      "Epoch 36/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0625 - acc: 0.4903 - val_loss: 1.0669 - val_acc: 0.5839\n",
      "Epoch 37/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0660 - acc: 0.4786 - val_loss: 1.0664 - val_acc: 0.5912\n",
      "Epoch 38/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0666 - acc: 0.4903 - val_loss: 1.0660 - val_acc: 0.5985\n",
      "Epoch 39/40\n",
      "257/257 [==============================] - 0s 2ms/step - loss: 1.0695 - acc: 0.4591 - val_loss: 1.0655 - val_acc: 0.5985\n",
      "Epoch 40/40\n",
      "257/257 [==============================] - 0s 1ms/step - loss: 1.0649 - acc: 0.4903 - val_loss: 1.0650 - val_acc: 0.5985\n",
      "153/153 [==============================] - 0s 361us/step\n",
      "Persons for train: \n",
      "[ 1.  2.  3.  5.  6.  9. 13. 16. 18. 19. 21.]\n",
      "Persons for val: \n",
      "[ 4.  7.  8. 10. 14.]\n",
      "Persons for test: \n",
      "[11. 12. 15. 17. 20. 22.]\n",
      "Train on 263 samples, validate on 126 samples\n",
      "Epoch 1/40\n",
      "263/263 [==============================] - 4s 15ms/step - loss: 1.1241 - acc: 0.2167 - val_loss: 1.1145 - val_acc: 0.1825\n",
      "Epoch 2/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1144 - acc: 0.2510 - val_loss: 1.1123 - val_acc: 0.1825\n",
      "Epoch 3/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1165 - acc: 0.2281 - val_loss: 1.1103 - val_acc: 0.1905\n",
      "Epoch 4/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1123 - acc: 0.2510 - val_loss: 1.1085 - val_acc: 0.2222\n",
      "Epoch 5/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1094 - acc: 0.2357 - val_loss: 1.1070 - val_acc: 0.2540\n",
      "Epoch 6/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1151 - acc: 0.2548 - val_loss: 1.1056 - val_acc: 0.2619\n",
      "Epoch 7/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1113 - acc: 0.2586 - val_loss: 1.1043 - val_acc: 0.2937\n",
      "Epoch 8/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1078 - acc: 0.2890 - val_loss: 1.1031 - val_acc: 0.3095\n",
      "Epoch 9/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1067 - acc: 0.3042 - val_loss: 1.1020 - val_acc: 0.3413\n",
      "Epoch 10/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1030 - acc: 0.3308 - val_loss: 1.1010 - val_acc: 0.3730\n",
      "Epoch 11/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1019 - acc: 0.3194 - val_loss: 1.0999 - val_acc: 0.3810\n",
      "Epoch 12/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1008 - acc: 0.3726 - val_loss: 1.0990 - val_acc: 0.3889\n",
      "Epoch 13/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.1038 - acc: 0.3384 - val_loss: 1.0981 - val_acc: 0.3889\n",
      "Epoch 14/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0909 - acc: 0.3536 - val_loss: 1.0972 - val_acc: 0.3889\n",
      "Epoch 15/40\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 1.0989 - acc: 0.3080 - val_loss: 1.0964 - val_acc: 0.4127\n",
      "Epoch 16/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0951 - acc: 0.3726 - val_loss: 1.0955 - val_acc: 0.4286\n",
      "Epoch 17/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0966 - acc: 0.3574 - val_loss: 1.0948 - val_acc: 0.4444\n",
      "Epoch 18/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0977 - acc: 0.3536 - val_loss: 1.0940 - val_acc: 0.4603\n",
      "Epoch 19/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0967 - acc: 0.2928 - val_loss: 1.0933 - val_acc: 0.4603\n",
      "Epoch 20/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0872 - acc: 0.4411 - val_loss: 1.0925 - val_acc: 0.4444\n",
      "Epoch 21/40\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 1.0884 - acc: 0.4144 - val_loss: 1.0918 - val_acc: 0.4444\n",
      "Epoch 22/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0868 - acc: 0.4259 - val_loss: 1.0911 - val_acc: 0.4444\n",
      "Epoch 23/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0921 - acc: 0.3802 - val_loss: 1.0905 - val_acc: 0.4444\n",
      "Epoch 24/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0911 - acc: 0.3840 - val_loss: 1.0898 - val_acc: 0.4524\n",
      "Epoch 25/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0846 - acc: 0.4449 - val_loss: 1.0891 - val_acc: 0.4524\n",
      "Epoch 26/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0882 - acc: 0.3878 - val_loss: 1.0885 - val_acc: 0.4524\n",
      "Epoch 27/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0877 - acc: 0.3726 - val_loss: 1.0879 - val_acc: 0.4444\n",
      "Epoch 28/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0870 - acc: 0.4068 - val_loss: 1.0873 - val_acc: 0.4524\n",
      "Epoch 29/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0874 - acc: 0.4449 - val_loss: 1.0867 - val_acc: 0.4524\n",
      "Epoch 30/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0853 - acc: 0.4183 - val_loss: 1.0860 - val_acc: 0.4524\n",
      "Epoch 31/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0817 - acc: 0.4639 - val_loss: 1.0854 - val_acc: 0.4603\n",
      "Epoch 32/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0808 - acc: 0.4601 - val_loss: 1.0849 - val_acc: 0.4683\n",
      "Epoch 33/40\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 1.0828 - acc: 0.4867 - val_loss: 1.0843 - val_acc: 0.4841\n",
      "Epoch 34/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0828 - acc: 0.4335 - val_loss: 1.0837 - val_acc: 0.4841\n",
      "Epoch 35/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0832 - acc: 0.4259 - val_loss: 1.0831 - val_acc: 0.4841\n",
      "Epoch 36/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0807 - acc: 0.4335 - val_loss: 1.0826 - val_acc: 0.4841\n",
      "Epoch 37/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0833 - acc: 0.4373 - val_loss: 1.0821 - val_acc: 0.4841\n",
      "Epoch 38/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0809 - acc: 0.4183 - val_loss: 1.0815 - val_acc: 0.4841\n",
      "Epoch 39/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0779 - acc: 0.4487 - val_loss: 1.0810 - val_acc: 0.4841\n",
      "Epoch 40/40\n",
      "263/263 [==============================] - 0s 2ms/step - loss: 1.0796 - acc: 0.4487 - val_loss: 1.0805 - val_acc: 0.4841\n",
      "158/158 [==============================] - 0s 428us/step\n",
      "Persons for train: \n",
      "[ 1.  2.  8.  9. 10. 12. 14. 17. 18. 19. 20.]\n",
      "Persons for val: \n",
      "[ 3. 11. 13. 15. 21.]\n",
      "Persons for test: \n",
      "[ 4.  5.  6.  7. 16. 22.]\n",
      "Train on 276 samples, validate on 111 samples\n",
      "Epoch 1/40\n",
      "276/276 [==============================] - 4s 15ms/step - loss: 1.0855 - acc: 0.4275 - val_loss: 1.0943 - val_acc: 0.4054\n",
      "Epoch 2/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0868 - acc: 0.4203 - val_loss: 1.0926 - val_acc: 0.4505\n",
      "Epoch 3/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0847 - acc: 0.4275 - val_loss: 1.0910 - val_acc: 0.4505\n",
      "Epoch 4/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0807 - acc: 0.4130 - val_loss: 1.0897 - val_acc: 0.4775\n",
      "Epoch 5/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0780 - acc: 0.4312 - val_loss: 1.0885 - val_acc: 0.4865\n",
      "Epoch 6/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0766 - acc: 0.4855 - val_loss: 1.0874 - val_acc: 0.4865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0774 - acc: 0.4457 - val_loss: 1.0864 - val_acc: 0.4865\n",
      "Epoch 8/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0719 - acc: 0.5181 - val_loss: 1.0855 - val_acc: 0.5045\n",
      "Epoch 9/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0734 - acc: 0.4964 - val_loss: 1.0847 - val_acc: 0.5045\n",
      "Epoch 10/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0700 - acc: 0.5036 - val_loss: 1.0838 - val_acc: 0.5045\n",
      "Epoch 11/40\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 1.0698 - acc: 0.5254 - val_loss: 1.0830 - val_acc: 0.5045\n",
      "Epoch 12/40\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 1.0658 - acc: 0.5217 - val_loss: 1.0823 - val_acc: 0.5045\n",
      "Epoch 13/40\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 1.0681 - acc: 0.5145 - val_loss: 1.0815 - val_acc: 0.5045\n",
      "Epoch 14/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0715 - acc: 0.5109 - val_loss: 1.0808 - val_acc: 0.5225\n",
      "Epoch 15/40\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 1.0682 - acc: 0.5435 - val_loss: 1.0801 - val_acc: 0.5315\n",
      "Epoch 16/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0637 - acc: 0.5507 - val_loss: 1.0794 - val_acc: 0.5315\n",
      "Epoch 17/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0631 - acc: 0.5000 - val_loss: 1.0788 - val_acc: 0.5405\n",
      "Epoch 18/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0613 - acc: 0.5362 - val_loss: 1.0782 - val_acc: 0.5405\n",
      "Epoch 19/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0567 - acc: 0.5688 - val_loss: 1.0776 - val_acc: 0.5405\n",
      "Epoch 20/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0659 - acc: 0.5543 - val_loss: 1.0770 - val_acc: 0.5405\n",
      "Epoch 21/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0647 - acc: 0.5543 - val_loss: 1.0764 - val_acc: 0.5405\n",
      "Epoch 22/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0620 - acc: 0.5471 - val_loss: 1.0759 - val_acc: 0.5495\n",
      "Epoch 23/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0546 - acc: 0.5833 - val_loss: 1.0753 - val_acc: 0.5495\n",
      "Epoch 24/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0577 - acc: 0.5797 - val_loss: 1.0748 - val_acc: 0.5495\n",
      "Epoch 25/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0587 - acc: 0.5761 - val_loss: 1.0743 - val_acc: 0.5495\n",
      "Epoch 26/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0579 - acc: 0.5870 - val_loss: 1.0738 - val_acc: 0.5405\n",
      "Epoch 27/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0588 - acc: 0.5725 - val_loss: 1.0733 - val_acc: 0.5405\n",
      "Epoch 28/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0568 - acc: 0.5688 - val_loss: 1.0728 - val_acc: 0.5405\n",
      "Epoch 29/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0554 - acc: 0.6196 - val_loss: 1.0723 - val_acc: 0.5495\n",
      "Epoch 30/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0538 - acc: 0.5725 - val_loss: 1.0718 - val_acc: 0.5495\n",
      "Epoch 31/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0522 - acc: 0.5580 - val_loss: 1.0713 - val_acc: 0.5495\n",
      "Epoch 32/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0485 - acc: 0.5942 - val_loss: 1.0708 - val_acc: 0.5495\n",
      "Epoch 33/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0533 - acc: 0.5978 - val_loss: 1.0703 - val_acc: 0.5495\n",
      "Epoch 34/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0453 - acc: 0.5978 - val_loss: 1.0699 - val_acc: 0.5495\n",
      "Epoch 35/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0520 - acc: 0.6014 - val_loss: 1.0694 - val_acc: 0.5766\n",
      "Epoch 36/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0470 - acc: 0.6159 - val_loss: 1.0689 - val_acc: 0.5766\n",
      "Epoch 37/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0415 - acc: 0.6377 - val_loss: 1.0685 - val_acc: 0.5766\n",
      "Epoch 38/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0450 - acc: 0.6051 - val_loss: 1.0680 - val_acc: 0.5766\n",
      "Epoch 39/40\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 1.0463 - acc: 0.6159 - val_loss: 1.0675 - val_acc: 0.5766\n",
      "Epoch 40/40\n",
      "276/276 [==============================] - 0s 2ms/step - loss: 1.0417 - acc: 0.6123 - val_loss: 1.0671 - val_acc: 0.5766\n",
      "160/160 [==============================] - 0s 414us/step\n",
      "[0.77622378 0.61971831 0.72058824 0.56976744 0.57051282 0.66666667\n",
      " 0.78064516 0.5751634  0.37974684 0.5125    ]\n"
     ]
    }
   ],
   "source": [
    "# parameters needed\n",
    "trials = 10\n",
    "epochs = 40\n",
    "batch_size = 30\n",
    "units = 192 # actually defind inside the funciton of the model - TODO - make it here\n",
    "path ='data/best'\n",
    "Files, T, max_dim = loadfromfolder()\n",
    "#Where Files are names, T - my division into frames and max_dim - feature dimensionality\n",
    "fRes = open(\"Results.txt\",\"a+\") \n",
    "fRes.write(\"Number of units in LSTM = %d,  Epochs =  %d and batch size =  %d \\r\\n\" % (units, epochs,batch_size))\n",
    "for file in range(0, len(Files)):\n",
    "    name = Files[file]\n",
    "    t = T[file]\n",
    "    dim = max_dim[file]\n",
    "    name_w_path = path +\"/\"+name\n",
    "    fRes.write(\"Experiment %d from file %s \\r\\n\" % (file,name))\n",
    "    # here all the routine in One cell \n",
    "    print(name_w_path)\n",
    "    X_all = pd.read_csv(name_w_path)\n",
    "    # the routine to run the same test N times, randomly shuffling the data\n",
    "    Accuracy  = np.zeros(shape=(trials))\n",
    "   \n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        modelname = str(file)+str(i)\n",
    "        X_train, Y_train, X_test, Y_test, X_val, Y_val, returned_train, returned_val = Random_Selection_train_val_test (X_all.values, 16)\n",
    "        #namePersons = str(i)+'_'+'file'+ name[:-4] +'.txt'\n",
    "        #np.savetxt(namePersons, returned_train, delimiter=',', fmt='%d')   # X is an array\n",
    "        X_train = NormalizeTowardsMean(X_train)\n",
    "        X_test = NormalizeTowardsMean(X_test)\n",
    "        X_val = NormalizeTowardsMean(X_val)\n",
    "        \n",
    "        Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "        Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "        Y_val_oh = convert_to_one_hot(Y_val, C = 3)\n",
    "        \n",
    "        X_train = Reshape_to_input(X_train, t, dim)\n",
    "        X_test = Reshape_to_input(X_test, t, dim)\n",
    "        X_val = Reshape_to_input(X_val, t, dim)\n",
    "        \n",
    "        model = Gait_model((X_train[0,:,:]))\n",
    "        \n",
    "        file_path = 'Models_LSTM/file_{}_model_wts.hdf5'.format(modelname)\n",
    "        callbacks = get_callbacks(filepath=file_path, patience = 35)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=myoptim, metrics=['accuracy'])\n",
    "              \n",
    "        \n",
    "        model.fit(X_train, Y_train_oh, epochs=epochs,\\\n",
    "                  batch_size=batch_size, shuffle=True, validation_data=(X_val, Y_val_oh), callbacks=callbacks)\n",
    "        \n",
    "        loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "        \n",
    "        Accuracy[i]=acc\n",
    "        fRes.write(\"Final accuracy per trial %f \\r\\n\" % (acc))\n",
    "        \n",
    "    print(Accuracy)\n",
    "    fRes.write(\"Final mean accuracy is: %f \\r\\n\" % (np.mean(Accuracy)))\n",
    "fRes.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fRes.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
