{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, RNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Data Loading, normalization, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reshape_to_input(X, T, Mat_dim):\n",
    "    \"\"\"\n",
    "    reshapes data\n",
    "    \"\"\"\n",
    "   \n",
    "    [Num_examples, dim] = X.shape\n",
    "    input_X = X[:,2:].reshape(Num_examples, Mat_dim, T)\n",
    "    \n",
    "    return input_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeTowardsMean(X):\n",
    "   # print(X.shape)\n",
    "    \"# Preprocessing: Subtract the mean feature - a question if it is feasible - all features are already in the same scale...\\n\",\n",
    "    # TODO: try -1 1 normalization\n",
    "    mean_feat = np.mean(X, axis=0, keepdims=True)\n",
    "   # print(mean_feat.shape)\n",
    "    X -= mean_feat\n",
    "    # Preprocessing: Divide by standard deviation. This ensures that each feature\\n\",\n",
    "    # has roughly the same scale.\\n\",\n",
    "    std_feat = np.std(X, axis=0, keepdims=True)\n",
    "    X /= std_feat\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_test = np.array([])\n",
    "    #print(X_train.shape)\n",
    "    for Person in range(0,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "        Persons_test = np.append(Persons_test, label, axis = 0)\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    returned_train = np.unique(Persons_train)\n",
    "    print('Persons for test: ')\n",
    "    print(np.unique(Persons_test))\n",
    "    return X_train, Y_train, X_test, Y_test, returned_train\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_val_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    X_val = np.array([])\n",
    "    Y_val = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_val = np.array([])\n",
    "    Persons_test = np.array([])\n",
    "    \n",
    "    train_N =int(np.ceil(N*2/3))\n",
    "    \n",
    "    for Person in range(0,train_N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    \n",
    "     \n",
    "    for Person in range(train_N,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_val= np.append(X_val,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_val = np.append(Y_val, X_all[[indexes], 0])\n",
    "        Persons_val = np.append(Persons_val, label, axis = 0)\n",
    "    X_val=X_val.reshape(Y_val.size, num_dim)  \n",
    "    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "        Persons_test = np.append(Persons_test, label, axis = 0)\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    returned_train = np.unique(Persons_train)\n",
    "    print('Persons for val: ')\n",
    "    print(np.unique(Persons_val))\n",
    "    returned_val = np.unique(Persons_val)\n",
    "    print('Persons for test: ')\n",
    "    print(np.unique(Persons_test))\n",
    "    return X_train, Y_train, X_test, Y_test, X_val, Y_val, returned_train, returned_val\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(inputY, C):\n",
    "    N= inputY.size\n",
    "    Y=np.zeros((N,C))\n",
    "    for i in range (0, inputY.size):\n",
    "        Y[i, int(inputY[i]-1)] = 1\n",
    "        \n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units = 128 \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = LSTM(units, return_sequences=True)( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(units, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my dataset and parameters from a file\n",
    "from data_utils import loadfromfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 SHORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units =  192\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = LSTM(units, return_sequences=False)( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    #X = LSTM(units, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN Loop through all the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters needed\n",
    "trials = 10\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "units = 128 # actually defind inside the funciton of the model - TODO - make it here\n",
    "path ='data/2_angles_only'\n",
    "Files, T, max_dim = loadfromfolder()\n",
    "#Where Files are names, T - my division into frames and max_dim - feature dimensionality\n",
    "fRes = open(\"Results.txt\",\"a+\") \n",
    "fRes.write(\"Number of units in LSTM = %d,  Epochs =  %d and batch size =  %d \\r\\n\" % (units, epochs,batch_size))\n",
    "for file in range(0, len(Files)):\n",
    "    name = Files[file]\n",
    "    t = T[file]\n",
    "    dim = max_dim[file]\n",
    "    name_w_path = path +\"/\"+name\n",
    "    fRes.write(\"Experiment %d from file %s \\r\\n\" % (file,name))\n",
    "    # here all the routine in One cell \n",
    "    X_all = pd.read_csv(name_w_path)\n",
    "    \n",
    "    # the routine to run the same test N times, randomly shuffling the data\n",
    "    Accuracy = X_train = np.zeros(shape=(trials))\n",
    "    for i in range(0,trials):\n",
    "        X_train, Y_train, X_test, Y_test, returned_train = Random_Selection_train_test (X_all.values, 14)\n",
    "        #fRes.write(returned_train) # the presons used for training\n",
    "        namePersons = str(i)+'_'+'file'+ name[:-4] +'.txt'\n",
    "        np.savetxt(namePersons, returned_train, delimiter=',', fmt='%d')   # X is an array\n",
    "        X_train = NormalizeTowardsMean(X_train)\n",
    "        X_test = NormalizeTowardsMean(X_test)\n",
    "        Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "        Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "        X_train = Reshape_to_input(X_train, t, dim)\n",
    "        X_test = Reshape_to_input(X_test, t, dim)\n",
    "        model = Gait_model((X_train[0,:,:]))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train_oh, epochs, batch_size, shuffle=True)\n",
    "        loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "        Accuracy[i]=acc\n",
    "        fRes.write(\"Final accuracy per trial %f \\r\\n\" % (acc))\n",
    "        \n",
    "    print(Accuracy)\n",
    "    fRes.write(\"Final mean accuracy is: %f \\r\\n\" % (np.mean(Accuracy)))\n",
    "fRes.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myoptim=Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filepath, save_best_only =True, monitor = 'val_loss', mode ='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [es, mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles_cycles_frames_10t_08f_angles_22persons.csv\n",
      "angles_cycles_frames_29t_08f_angles_22persons.csv\n",
      "angles_cycles_frames_15t_08f_angles_22persons.csv\n",
      "covmat_cycles_frames_12t_36f_angles_22persons.csv\n",
      "covmat_cycles_frames_08t_36f_angles_22persons.csv\n",
      "angles_cycles_frames_06t_08f_angles_22persons.csv\n",
      "covmat_cycles_frames_06t_36f_angles_22persons.csv\n",
      "Persons for train: \n",
      "[ 1.  2.  7. 12. 13. 14. 17. 18. 19. 21.]\n",
      "Persons for val: \n",
      "[ 8. 10. 15. 20.]\n",
      "Persons for test: \n",
      "[ 3.  4.  5.  6.  9. 11. 16. 22.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khokhlov/virtenv/ml36/lib/python3.5/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 247 samples, validate on 84 samples\n",
      "Epoch 1/30\n",
      "247/247 [==============================] - 17s 67ms/step - loss: 0.9983 - acc: 0.4656 - val_loss: 0.9078 - val_acc: 0.6310\n",
      "Epoch 2/30\n",
      "247/247 [==============================] - 0s 961us/step - loss: 0.8435 - acc: 0.6073 - val_loss: 0.7696 - val_acc: 0.7024\n",
      "Epoch 3/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.7780 - acc: 0.6275 - val_loss: 0.8027 - val_acc: 0.6429\n",
      "Epoch 4/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.6879 - acc: 0.7166 - val_loss: 0.7288 - val_acc: 0.7143\n",
      "Epoch 5/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.6262 - acc: 0.7328 - val_loss: 0.6861 - val_acc: 0.7857\n",
      "Epoch 6/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.5510 - acc: 0.7814 - val_loss: 0.6534 - val_acc: 0.7619\n",
      "Epoch 7/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.4964 - acc: 0.8219 - val_loss: 0.7178 - val_acc: 0.7500\n",
      "Epoch 8/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.4377 - acc: 0.8138 - val_loss: 0.6749 - val_acc: 0.7262\n",
      "Epoch 9/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.3753 - acc: 0.8340 - val_loss: 0.7589 - val_acc: 0.7381\n",
      "Epoch 10/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.3430 - acc: 0.8502 - val_loss: 0.7959 - val_acc: 0.6667\n",
      "Epoch 11/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.3207 - acc: 0.8745 - val_loss: 0.7573 - val_acc: 0.7262\n",
      "Epoch 12/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.2946 - acc: 0.8907 - val_loss: 0.8702 - val_acc: 0.6905\n",
      "Epoch 13/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.2802 - acc: 0.9069 - val_loss: 0.8710 - val_acc: 0.7024\n",
      "Epoch 14/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.2414 - acc: 0.9069 - val_loss: 0.9297 - val_acc: 0.6786\n",
      "Epoch 15/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.2004 - acc: 0.9231 - val_loss: 0.9692 - val_acc: 0.7024\n",
      "Epoch 16/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1917 - acc: 0.9433 - val_loss: 1.1330 - val_acc: 0.6548\n",
      "Epoch 17/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1909 - acc: 0.9352 - val_loss: 1.0164 - val_acc: 0.7500\n",
      "Epoch 18/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1721 - acc: 0.9312 - val_loss: 1.2507 - val_acc: 0.6667\n",
      "Epoch 19/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1722 - acc: 0.9433 - val_loss: 1.1071 - val_acc: 0.7381\n",
      "Epoch 20/30\n",
      "247/247 [==============================] - 0s 988us/step - loss: 0.1527 - acc: 0.9393 - val_loss: 1.2009 - val_acc: 0.7024\n",
      "Epoch 21/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1306 - acc: 0.9636 - val_loss: 1.3413 - val_acc: 0.6786\n",
      "Epoch 22/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1332 - acc: 0.9514 - val_loss: 1.2749 - val_acc: 0.7262\n",
      "Epoch 23/30\n",
      "247/247 [==============================] - 0s 869us/step - loss: 0.1159 - acc: 0.9717 - val_loss: 1.2710 - val_acc: 0.7024\n",
      "Epoch 24/30\n",
      "247/247 [==============================] - 0s 977us/step - loss: 0.0963 - acc: 0.9717 - val_loss: 1.2783 - val_acc: 0.7381\n",
      "Epoch 25/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1021 - acc: 0.9717 - val_loss: 1.3543 - val_acc: 0.7024\n",
      "Epoch 26/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1105 - acc: 0.9595 - val_loss: 1.4058 - val_acc: 0.7143\n",
      "Epoch 27/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.1254 - acc: 0.9514 - val_loss: 1.3519 - val_acc: 0.7381\n",
      "Epoch 28/30\n",
      "247/247 [==============================] - 0s 1ms/step - loss: 0.0990 - acc: 0.9636 - val_loss: 1.4987 - val_acc: 0.6905\n",
      "Epoch 29/30\n",
      "247/247 [==============================] - 0s 771us/step - loss: 0.0844 - acc: 0.9717 - val_loss: 1.3953 - val_acc: 0.7500\n",
      "Epoch 30/30\n",
      "247/247 [==============================] - 0s 867us/step - loss: 0.0718 - acc: 0.9717 - val_loss: 1.5418 - val_acc: 0.7024\n",
      "216/216 [==============================] - 0s 264us/step\n",
      "Persons for train: \n",
      "[ 5.  6.  7.  9. 10. 14. 15. 16. 17. 22.]\n",
      "Persons for val: \n",
      "[11. 12. 18. 20.]\n",
      "Persons for test: \n",
      "[ 1.  2.  3.  4.  8. 13. 19. 21.]\n",
      "Train on 258 samples, validate on 98 samples\n",
      "Epoch 1/30\n",
      "258/258 [==============================] - 17s 67ms/step - loss: 1.0066 - acc: 0.4651 - val_loss: 0.9683 - val_acc: 0.5102\n",
      "Epoch 2/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.7970 - acc: 0.6473 - val_loss: 0.9499 - val_acc: 0.5714\n",
      "Epoch 3/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.7044 - acc: 0.6783 - val_loss: 0.7972 - val_acc: 0.6837\n",
      "Epoch 4/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.6448 - acc: 0.7287 - val_loss: 0.7137 - val_acc: 0.6837\n",
      "Epoch 5/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.5872 - acc: 0.7713 - val_loss: 0.6720 - val_acc: 0.7551\n",
      "Epoch 6/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.5320 - acc: 0.7946 - val_loss: 0.6863 - val_acc: 0.6939\n",
      "Epoch 7/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.4955 - acc: 0.8178 - val_loss: 0.6647 - val_acc: 0.7143\n",
      "Epoch 8/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.4471 - acc: 0.8411 - val_loss: 0.6694 - val_acc: 0.6837\n",
      "Epoch 9/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.4364 - acc: 0.8178 - val_loss: 0.6847 - val_acc: 0.6837\n",
      "Epoch 10/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.4064 - acc: 0.8411 - val_loss: 0.7233 - val_acc: 0.7041\n",
      "Epoch 11/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.4001 - acc: 0.8372 - val_loss: 0.7743 - val_acc: 0.6939\n",
      "Epoch 12/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.3875 - acc: 0.8527 - val_loss: 0.7528 - val_acc: 0.7041\n",
      "Epoch 13/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.3485 - acc: 0.8643 - val_loss: 0.8003 - val_acc: 0.7143\n",
      "Epoch 14/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.3259 - acc: 0.8837 - val_loss: 0.7364 - val_acc: 0.7143\n",
      "Epoch 15/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.3133 - acc: 0.8837 - val_loss: 0.8334 - val_acc: 0.7143\n",
      "Epoch 16/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.3076 - acc: 0.8798 - val_loss: 0.7758 - val_acc: 0.7245\n",
      "Epoch 17/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2984 - acc: 0.8837 - val_loss: 0.8062 - val_acc: 0.7143\n",
      "Epoch 18/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2799 - acc: 0.8915 - val_loss: 0.8981 - val_acc: 0.7041\n",
      "Epoch 19/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2926 - acc: 0.8915 - val_loss: 0.8846 - val_acc: 0.6939\n",
      "Epoch 20/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2509 - acc: 0.9031 - val_loss: 0.8850 - val_acc: 0.7041\n",
      "Epoch 21/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2488 - acc: 0.9147 - val_loss: 0.9777 - val_acc: 0.7041\n",
      "Epoch 22/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2428 - acc: 0.9070 - val_loss: 1.0135 - val_acc: 0.7143\n",
      "Epoch 23/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2354 - acc: 0.9070 - val_loss: 0.9582 - val_acc: 0.7041\n",
      "Epoch 24/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2432 - acc: 0.9186 - val_loss: 1.0642 - val_acc: 0.7041\n",
      "Epoch 25/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2480 - acc: 0.9109 - val_loss: 0.9198 - val_acc: 0.7245\n",
      "Epoch 26/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2099 - acc: 0.9380 - val_loss: 1.0805 - val_acc: 0.7041\n",
      "Epoch 27/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2166 - acc: 0.9147 - val_loss: 0.9813 - val_acc: 0.7143\n",
      "Epoch 28/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2029 - acc: 0.9302 - val_loss: 1.0193 - val_acc: 0.7245\n",
      "Epoch 29/30\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.1907 - acc: 0.9264 - val_loss: 1.1166 - val_acc: 0.7143\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/258 [==============================] - 0s 1ms/step - loss: 0.1760 - acc: 0.9302 - val_loss: 1.0783 - val_acc: 0.7143\n",
      "191/191 [==============================] - 0s 251us/step\n",
      "Persons for train: \n",
      "[ 1.  4.  6.  7. 11. 13. 14. 16. 18. 21.]\n",
      "Persons for val: \n",
      "[ 3.  5.  9. 20.]\n",
      "Persons for test: \n",
      "[ 2.  8. 10. 12. 15. 17. 19. 22.]\n",
      "Train on 242 samples, validate on 102 samples\n",
      "Epoch 1/30\n",
      "242/242 [==============================] - 17s 69ms/step - loss: 1.0215 - acc: 0.4793 - val_loss: 0.9498 - val_acc: 0.5490\n",
      "Epoch 2/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8901 - acc: 0.5537 - val_loss: 0.9167 - val_acc: 0.5588\n",
      "Epoch 3/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8093 - acc: 0.6529 - val_loss: 0.8833 - val_acc: 0.5686\n",
      "Epoch 4/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7200 - acc: 0.6818 - val_loss: 0.9288 - val_acc: 0.5588\n",
      "Epoch 5/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6803 - acc: 0.7066 - val_loss: 0.8541 - val_acc: 0.6667\n",
      "Epoch 6/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6138 - acc: 0.7479 - val_loss: 0.8748 - val_acc: 0.6275\n",
      "Epoch 7/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5873 - acc: 0.7645 - val_loss: 0.8205 - val_acc: 0.6765\n",
      "Epoch 8/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5449 - acc: 0.7769 - val_loss: 0.8021 - val_acc: 0.7059\n",
      "Epoch 9/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4976 - acc: 0.8099 - val_loss: 0.7936 - val_acc: 0.7157\n",
      "Epoch 10/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4739 - acc: 0.8264 - val_loss: 0.7710 - val_acc: 0.7059\n",
      "Epoch 11/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4672 - acc: 0.8306 - val_loss: 0.7180 - val_acc: 0.7255\n",
      "Epoch 12/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - acc: 0.8430 - val_loss: 0.8202 - val_acc: 0.7059\n",
      "Epoch 13/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - acc: 0.8347 - val_loss: 0.6736 - val_acc: 0.7353\n",
      "Epoch 14/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - acc: 0.8554 - val_loss: 0.9026 - val_acc: 0.6863\n",
      "Epoch 15/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3895 - acc: 0.8678 - val_loss: 0.7171 - val_acc: 0.6961\n",
      "Epoch 16/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3537 - acc: 0.8843 - val_loss: 0.7600 - val_acc: 0.7157\n",
      "Epoch 17/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3321 - acc: 0.8967 - val_loss: 0.8241 - val_acc: 0.6961\n",
      "Epoch 18/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3097 - acc: 0.8843 - val_loss: 0.7979 - val_acc: 0.7059\n",
      "Epoch 19/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2946 - acc: 0.8926 - val_loss: 0.8098 - val_acc: 0.6863\n",
      "Epoch 20/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2876 - acc: 0.8967 - val_loss: 0.8648 - val_acc: 0.6961\n",
      "Epoch 21/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2717 - acc: 0.9091 - val_loss: 0.8715 - val_acc: 0.6765\n",
      "Epoch 22/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2680 - acc: 0.9174 - val_loss: 0.8862 - val_acc: 0.6863\n",
      "Epoch 23/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2538 - acc: 0.9174 - val_loss: 0.8462 - val_acc: 0.7059\n",
      "Epoch 24/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2401 - acc: 0.9215 - val_loss: 1.0191 - val_acc: 0.6863\n",
      "Epoch 25/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2282 - acc: 0.9174 - val_loss: 0.8449 - val_acc: 0.6863\n",
      "Epoch 26/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2262 - acc: 0.9298 - val_loss: 0.9298 - val_acc: 0.6765\n",
      "Epoch 27/30\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.2276 - acc: 0.9215 - val_loss: 0.8217 - val_acc: 0.7059\n",
      "Epoch 28/30\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.2059 - acc: 0.9174 - val_loss: 1.1300 - val_acc: 0.6569\n",
      "Epoch 29/30\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.2155 - acc: 0.9339 - val_loss: 0.8018 - val_acc: 0.6667\n",
      "Epoch 30/30\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2149 - acc: 0.9256 - val_loss: 0.9260 - val_acc: 0.6863\n",
      "203/203 [==============================] - 0s 281us/step\n",
      "[0.64814815 0.70157068 0.72906404]\n",
      "Persons for train: \n",
      "[ 1.  6.  7. 10. 11. 14. 15. 17. 18. 19.]\n",
      "Persons for val: \n",
      "[ 3. 13. 16. 22.]\n",
      "Persons for test: \n",
      "[ 2.  4.  5.  8.  9. 12. 20. 21.]\n",
      "Train on 238 samples, validate on 91 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# parameters needed\n",
    "trials = 3\n",
    "epochs = 25\n",
    "batch_size = 30\n",
    "units = 192 # actually defind inside the funciton of the model - TODO - make it here\n",
    "path ='data/2_angles_only'\n",
    "Files, T, max_dim = loadfromfolder()\n",
    "#Where Files are names, T - my division into frames and max_dim - feature dimensionality\n",
    "fRes = open(\"Results.txt\",\"a+\") \n",
    "fRes.write(\"Number of units in LSTM = %d,  Epochs =  %d and batch size =  %d \\r\\n\" % (units, epochs,batch_size))\n",
    "for file in range(0, len(Files)):\n",
    "    name = Files[file]\n",
    "    t = T[file]\n",
    "    dim = max_dim[file]\n",
    "    name_w_path = path +\"/\"+name\n",
    "    fRes.write(\"Experiment %d from file %s \\r\\n\" % (file,name))\n",
    "    # here all the routine in One cell \n",
    "    X_all = pd.read_csv(name_w_path)\n",
    "    # the routine to run the same test N times, randomly shuffling the data\n",
    "    Accuracy  = np.zeros(shape=(trials))\n",
    "   \n",
    "    \n",
    "    for i in range(0,trials):\n",
    "        X_train, Y_train, X_test, Y_test, X_val, Y_val, returned_train, returned_val = Random_Selection_train_val_test (X_all.values, 14)\n",
    "        #namePersons = str(i)+'_'+'file'+ name[:-4] +'.txt'\n",
    "        #np.savetxt(namePersons, returned_train, delimiter=',', fmt='%d')   # X is an array\n",
    "        X_train = NormalizeTowardsMean(X_train)\n",
    "        X_test = NormalizeTowardsMean(X_test)\n",
    "        X_val = NormalizeTowardsMean(X_val)\n",
    "        \n",
    "        Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "        Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "        Y_val_oh = convert_to_one_hot(Y_val, C = 3)\n",
    "        \n",
    "        X_train = Reshape_to_input(X_train, t, dim)\n",
    "        X_test = Reshape_to_input(X_test, t, dim)\n",
    "        X_val = Reshape_to_input(X_val, t, dim)\n",
    "        \n",
    "        model = Gait_model((X_train[0,:,:]))\n",
    "        \n",
    "        file_path = 'Models/file_{}_model_wts.hdf5'.format(file)\n",
    "        callbacks = get_callbacks(filepath=file_path, patience = 35)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=myoptim, metrics=['accuracy'])\n",
    "              \n",
    "        \n",
    "        model.fit(X_train, Y_train_oh, epochs, batch_size, shuffle=True, validation_data=(X_val, Y_val_oh))\n",
    "        \n",
    "        loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "        \n",
    "        Accuracy[i]=acc\n",
    "        fRes.write(\"Final accuracy per trial %f \\r\\n\" % (acc))\n",
    "        \n",
    "    print(Accuracy)\n",
    "    fRes.write(\"Final mean accuracy is: %f \\r\\n\" % (np.mean(Accuracy)))\n",
    "fRes.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fRes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0895a7355d43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfRes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fRes' is not defined"
     ]
    }
   ],
   "source": [
    "fRes.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
