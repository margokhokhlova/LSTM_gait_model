{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, RNN, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Data Loading, normalization, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reshape_to_input(X, T, Mat_dim):\n",
    "    \"\"\"\n",
    "    reshapes data\n",
    "    \"\"\"\n",
    "   \n",
    "    [Num_examples, dim] = X.shape\n",
    "    input_X = X[:,2:].reshape(Num_examples, Mat_dim, T)\n",
    "    \n",
    "    return input_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeTowardsMean(X):\n",
    "   # print(X.shape)\n",
    "    \"# Preprocessing: Subtract the mean feature - a question if it is feasible - all features are already in the same scale...\\n\",\n",
    "    # TODO: try -1 1 normalization\n",
    "    mean_feat = np.mean(X, axis=0, keepdims=True)\n",
    "   # print(mean_feat.shape)\n",
    "    X -= mean_feat\n",
    "    # Preprocessing: Divide by standard deviation. This ensures that each feature\\n\",\n",
    "    # has roughly the same scale.\\n\",\n",
    "    std_feat = np.std(X, axis=0, keepdims=True)\n",
    "    X /= std_feat\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_test = np.array([])\n",
    "    #print(X_train.shape)\n",
    "    for Person in range(0,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "        Persons_test = np.append(Persons_test, label, axis = 0)\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    print('Persons for test: ')\n",
    "    print(np.unique(Persons_test))\n",
    "    return X_train, Y_train, X_test, Y_test, Persons_train\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_val (X, Y, N, Person_labels):\n",
    "    uniquePersons = np.unique(Person_labels)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X)[1]\n",
    "    X_train = np.array([])\n",
    "    Y_train = np.array([])\n",
    "    X_val = np.array([])\n",
    "    Y_val = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_val = np.array([])\n",
    "    for Person in range(0,N):\n",
    "        indexes = np.where(Person_labels==uniquePersons[Person])\n",
    "        Person_X_val = X[[indexes], :]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, Y[indexes])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim) \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(Person_labels==uniquePersons[Person])\n",
    "        Person_X_val = X[[indexes], :]\n",
    "        X_val= np.append(X_val,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_val = np.append(Y_val, Y[indexes])\n",
    "        Persons_val = np.append(Persons_val, label, axis = 0)\n",
    "    X_val=X_val.reshape(Y_val.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    print(Y_train)\n",
    "    print('Persons for validation: ')\n",
    "    print(np.unique(Persons_val))\n",
    "    print(Y_val)\n",
    "    print(X_train.shape)\n",
    "    print(X_val.shape)\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Selection_train_val_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    X_val = np.array([])\n",
    "    Y_val = np.array([])\n",
    "    Persons_train = np.array([])\n",
    "    Persons_val = np.array([])\n",
    "    Persons_test = np.array([])\n",
    "    \n",
    "    train_N =int(np.ceil(N*2/3))\n",
    "    \n",
    "    for Person in range(0,train_N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "        Persons_train = np.append(Persons_train, label, axis = 0)\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    \n",
    "     \n",
    "    for Person in range(train_N,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_val= np.append(X_val,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_val = np.append(Y_val, X_all[[indexes], 0])\n",
    "        Persons_val = np.append(Persons_val, label, axis = 0)\n",
    "    X_val=X_val.reshape(Y_val.size, num_dim)  \n",
    "    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "        Persons_test = np.append(Persons_test, label, axis = 0)\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim) \n",
    "    print('Persons for train: ')\n",
    "    print(np.unique(Persons_train))\n",
    "    returned_train = np.unique(Persons_train)\n",
    "    print('Persons for val: ')\n",
    "    print(np.unique(Persons_val))\n",
    "    returned_val = np.unique(Persons_val)\n",
    "    print('Persons for test: ')\n",
    "    print(np.unique(Persons_test))\n",
    "    return X_train, Y_train, X_test, Y_test, X_val, Y_val, returned_train, returned_val\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(inputY, C):\n",
    "    N= inputY.size\n",
    "    Y=np.zeros((N,C))\n",
    "    for i in range (0, inputY.size):\n",
    "        Y[i, int(inputY[i]-1)] = 1\n",
    "        \n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units = 128 \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = Bidirectional(LSTM(units, return_sequences=True))( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    X = Bidirectional(LSTM(units, return_sequences=False))(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my dataset and parameters from a file\n",
    "from data_utils import loadfromfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myoptim=Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filepath, save_best_only =True, monitor = 'val_loss', mode ='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min') #epsilon\n",
    "    return [es, mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles_cycles_frames_15t_08f_angles_22persons.csv\n",
      "Persons for train: \n",
      "[ 1.  2.  4.  5.  6.  7.  9. 10. 12. 13. 14. 15. 16. 18. 19. 21.]\n",
      "Persons for test: \n",
      "[ 3.  8. 11. 17. 20. 22.]\n",
      "Persons for train: \n",
      "[ 4.  5.  7.  9. 10. 12. 13. 14. 16. 18. 21.]\n",
      "Persons for test: \n",
      "[ 1.  2.  6. 15. 19.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khokhlov/virtenv/ml36/lib/python3.5/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283 samples, validate on 115 samples\n",
      "Epoch 1/25\n",
      "283/283 [==============================] - 7s 25ms/step - loss: 1.0285 - acc: 0.4488 - val_loss: 0.9573 - val_acc: 0.5913\n",
      "Epoch 2/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.8814 - acc: 0.6572 - val_loss: 0.8396 - val_acc: 0.6435\n",
      "Epoch 3/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.7426 - acc: 0.6996 - val_loss: 0.7399 - val_acc: 0.6870\n",
      "Epoch 4/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.6439 - acc: 0.7703 - val_loss: 0.6569 - val_acc: 0.7652\n",
      "Epoch 5/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.5422 - acc: 0.8057 - val_loss: 0.5944 - val_acc: 0.7826\n",
      "Epoch 6/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.5109 - acc: 0.7986 - val_loss: 0.5507 - val_acc: 0.7478\n",
      "Epoch 7/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.4450 - acc: 0.8516 - val_loss: 0.5254 - val_acc: 0.7652\n",
      "Epoch 8/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8551 - val_loss: 0.4953 - val_acc: 0.7652\n",
      "Epoch 9/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.3504 - acc: 0.8834 - val_loss: 0.4877 - val_acc: 0.7913\n",
      "Epoch 10/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.3373 - acc: 0.8869 - val_loss: 0.4789 - val_acc: 0.7826\n",
      "Epoch 11/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.3114 - acc: 0.8905 - val_loss: 0.4675 - val_acc: 0.8087\n",
      "Epoch 12/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.2650 - acc: 0.9011 - val_loss: 0.4700 - val_acc: 0.8261\n",
      "Epoch 13/25\n",
      "283/283 [==============================] - 1s 4ms/step - loss: 0.2328 - acc: 0.9399 - val_loss: 0.4680 - val_acc: 0.8261\n",
      "Epoch 14/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.2059 - acc: 0.9364 - val_loss: 0.4689 - val_acc: 0.8261\n",
      "Epoch 15/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1904 - acc: 0.9470 - val_loss: 0.5142 - val_acc: 0.8087\n",
      "Epoch 16/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1683 - acc: 0.9647 - val_loss: 0.4884 - val_acc: 0.8348\n",
      "Epoch 17/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1615 - acc: 0.9647 - val_loss: 0.5178 - val_acc: 0.8087\n",
      "Epoch 18/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1505 - acc: 0.9576 - val_loss: 0.5403 - val_acc: 0.8087\n",
      "Epoch 19/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1374 - acc: 0.9611 - val_loss: 0.5297 - val_acc: 0.8087\n",
      "Epoch 20/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1221 - acc: 0.9647 - val_loss: 0.5912 - val_acc: 0.7913\n",
      "Epoch 21/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.1074 - acc: 0.9788 - val_loss: 0.5772 - val_acc: 0.8261\n",
      "Epoch 22/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.0940 - acc: 0.9753 - val_loss: 0.6036 - val_acc: 0.8087\n",
      "Epoch 23/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.0871 - acc: 0.9859 - val_loss: 0.5969 - val_acc: 0.8000\n",
      "Epoch 24/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.0794 - acc: 0.9788 - val_loss: 0.6026 - val_acc: 0.8087\n",
      "Epoch 25/25\n",
      "283/283 [==============================] - 1s 3ms/step - loss: 0.0720 - acc: 0.9859 - val_loss: 0.6293 - val_acc: 0.8174\n",
      "149/149 [==============================] - 0s 605us/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9d66c7ec8a6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mprobability_assigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_oh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#verbose=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_assigned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mAccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "# parameters needed\n",
    "trials = 1\n",
    "epochs = 25\n",
    "batch_size = 25\n",
    "units = 192 # actually defind inside the funciton of the model - TODO - make it here\n",
    "path ='data/best'\n",
    "Files, T, max_dim = loadfromfolder()\n",
    "#Where Files are names, T - my division into frames and max_dim - feature dimensionality\n",
    "#fRes = open(\"Results.txt\",\"a+\") \n",
    "#fRes.write(\"Number of units in LSTM = %d,  Epochs =  %d and batch size =  %d \\r\\n\" % (units, epochs,batch_size))\n",
    "for file in range(0, len(Files)):\n",
    "    name = Files[file]\n",
    "    t = T[file]\n",
    "    dim = max_dim[file]\n",
    "    name_w_path = path +\"/\"+name\n",
    "    #fRes.write(\"Experiment %d from file %s \\r\\n\" % (file,name))\n",
    "    # here all the routine in One cell \n",
    "    X_all = pd.read_csv(name_w_path)\n",
    "    # the routine to run the same test N times, randomly shuffling the data\n",
    "    Accuracy  = np.zeros(shape=(trials))\n",
    "    X_train_all, Y_train_all, X_test, Y_test, returned_train = Random_Selection_train_test (X_all.values, 16)\n",
    "    Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "    X_test = Reshape_to_input(X_test, t, dim)\n",
    "    for i in range(0,trials):\n",
    "        modelname = str(file)+str(i)\n",
    "        \n",
    "        X_train, Y_train, X_val, Y_val, returned_train = Random_Selection_train_test (X_train_all, 11)\n",
    "\n",
    "        Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "        Y_val_oh = convert_to_one_hot(Y_val, C = 3)\n",
    "        \n",
    "        X_train = Reshape_to_input(X_train, t, dim)\n",
    "        X_val = Reshape_to_input(X_val, t, dim)\n",
    "        \n",
    "        \n",
    "        model = Gait_model((X_train[0,:,:]))\n",
    "        \n",
    "        file_path = 'models/file_{}_model_wts.hdf5'.format(modelname)\n",
    "        callbacks = get_callbacks(filepath=file_path, patience = 35)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=myoptim, metrics=['accuracy'])\n",
    "              \n",
    "              \n",
    "        \n",
    "        model.fit(X_train, Y_train_oh, epochs=epochs,\\\n",
    "                  batch_size=batch_size, shuffle=True, validation_data=(X_val, Y_val_oh), callbacks=callbacks)\n",
    "        \n",
    "        loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "        \n",
    "        probability_assigned = model.predict(X_test) #verbose=1\n",
    "        print(probability_assigned)\n",
    "        Accuracy[i]=acc\n",
    "        #fRes.write(\"Final accuracy per trial %f \\r\\n\" % (acc))\n",
    "        \n",
    "    print(Accuracy)\n",
    "    #fRes.write(\"Final mean accuracy is: %f \\r\\n\" % (np.mean(Accuracy)))\n",
    "#fRes.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.98949707e-01 8.70735676e-04 1.79563649e-04]\n",
      " [9.98317719e-01 1.54931098e-03 1.33060967e-04]\n",
      " [9.98885572e-01 8.92005977e-04 2.22434799e-04]\n",
      " [9.98817742e-01 9.85471415e-04 1.96671070e-04]\n",
      " [9.99099016e-01 7.25850288e-04 1.75211579e-04]\n",
      " [9.95639563e-01 3.58978566e-03 7.70696439e-04]\n",
      " [2.67765630e-04 9.93724167e-01 6.00807322e-03]\n",
      " [3.91293142e-04 9.95800793e-01 3.80787882e-03]\n",
      " [5.32922801e-04 9.98729765e-01 7.37328839e-04]\n",
      " [6.88194923e-05 9.97945964e-01 1.98531384e-03]\n",
      " [1.93786123e-04 9.97772992e-01 2.03323714e-03]\n",
      " [1.29415275e-04 9.99122918e-01 7.47596147e-04]\n",
      " [1.14455152e-05 3.24362703e-02 9.67552304e-01]\n",
      " [4.77309504e-06 6.94034342e-03 9.93054926e-01]\n",
      " [2.81091707e-05 1.77904710e-01 8.22067201e-01]\n",
      " [4.12577583e-06 1.01512773e-02 9.89844561e-01]\n",
      " [1.90592004e-06 7.24747404e-03 9.92750645e-01]\n",
      " [3.62772789e-06 8.73268023e-03 9.91263747e-01]\n",
      " [5.44257944e-07 6.81274279e-04 9.99318242e-01]\n",
      " [9.92303908e-01 5.87433856e-03 1.82169257e-03]\n",
      " [9.98090684e-01 1.18078059e-03 7.28475221e-04]\n",
      " [9.98846769e-01 9.18401463e-04 2.34756852e-04]\n",
      " [9.98514235e-01 1.03329902e-03 4.52397304e-04]\n",
      " [9.98715401e-01 9.87994834e-04 2.96526792e-04]\n",
      " [9.98663068e-01 9.17244353e-04 4.19599994e-04]\n",
      " [2.47317227e-03 9.95718062e-01 1.80870760e-03]\n",
      " [1.68192096e-03 9.93571162e-01 4.74697445e-03]\n",
      " [1.84566772e-03 9.91111577e-01 7.04280101e-03]\n",
      " [3.60972277e-04 9.95433211e-01 4.20580851e-03]\n",
      " [3.74316471e-03 9.95065510e-01 1.19126670e-03]\n",
      " [4.64723707e-04 9.94800806e-01 4.73447330e-03]\n",
      " [1.61090051e-03 9.97279644e-01 1.10940170e-03]\n",
      " [8.08996789e-04 9.96871293e-01 2.31968774e-03]\n",
      " [1.77550642e-03 9.97641087e-01 5.83414920e-04]\n",
      " [1.20318076e-03 9.98138070e-01 6.58747798e-04]\n",
      " [1.38596178e-03 9.95398462e-01 3.21557466e-03]\n",
      " [1.23921973e-05 1.18639311e-02 9.88123715e-01]\n",
      " [3.90989764e-04 9.56952155e-01 4.26568650e-02]\n",
      " [1.35161405e-04 5.58071248e-02 9.44057703e-01]\n",
      " [4.91185119e-06 5.42371860e-03 9.94571388e-01]\n",
      " [4.55161199e-07 1.71950611e-04 9.99827623e-01]\n",
      " [2.78578909e-06 3.17458669e-03 9.96822596e-01]\n",
      " [3.40667725e-07 1.11741669e-04 9.99887943e-01]\n",
      " [6.12028145e-07 2.60153291e-04 9.99739230e-01]\n",
      " [4.59518049e-07 1.95404835e-04 9.99804199e-01]\n",
      " [1.43152965e-06 1.28792471e-03 9.98710632e-01]\n",
      " [3.62375033e-07 1.03305982e-04 9.99896288e-01]\n",
      " [8.62292836e-06 1.17931506e-02 9.88198221e-01]\n",
      " [9.95853782e-01 2.74149422e-03 1.40470953e-03]\n",
      " [9.98182774e-01 1.13491586e-03 6.82298618e-04]\n",
      " [9.67066526e-01 2.09275987e-02 1.20058060e-02]\n",
      " [9.98223484e-01 1.10161561e-03 6.74983894e-04]\n",
      " [9.89523590e-01 8.29172414e-03 2.18469370e-03]\n",
      " [9.98174787e-01 1.14465470e-03 6.80541154e-04]\n",
      " [9.97599900e-01 1.68028602e-03 7.19869277e-04]\n",
      " [9.96518850e-01 1.99888600e-03 1.48226961e-03]\n",
      " [9.91639555e-01 5.05760452e-03 3.30290734e-03]\n",
      " [8.60423893e-02 7.76872158e-01 1.37085482e-01]\n",
      " [6.10255776e-03 9.92137372e-01 1.76003156e-03]\n",
      " [2.35793856e-03 9.95998979e-01 1.64309412e-03]\n",
      " [1.70867701e-04 9.89804864e-01 1.00242682e-02]\n",
      " [2.72103702e-04 9.98348117e-01 1.37978711e-03]\n",
      " [3.32668424e-02 9.61903989e-01 4.82913340e-03]\n",
      " [4.65664431e-04 9.86676455e-01 1.28578302e-02]\n",
      " [8.85874208e-04 9.97127712e-01 1.98638020e-03]\n",
      " [2.75330327e-04 9.31718886e-01 6.80057406e-02]\n",
      " [2.40715744e-04 9.37000990e-01 6.27582893e-02]\n",
      " [6.29136339e-04 9.86479819e-01 1.28910104e-02]\n",
      " [4.44496691e-05 1.12212151e-01 8.87743413e-01]\n",
      " [2.47890439e-05 1.56735778e-02 9.84301627e-01]\n",
      " [9.76330717e-04 6.65678203e-01 3.33345443e-01]\n",
      " [4.48642008e-04 9.48536456e-01 5.10148555e-02]\n",
      " [4.80701041e-04 6.26553416e-01 3.72965902e-01]\n",
      " [1.35307186e-04 9.90593612e-01 9.27113369e-03]\n",
      " [9.99461949e-01 4.25906008e-04 1.12098183e-04]\n",
      " [9.97553051e-01 2.17137276e-03 2.75611063e-04]\n",
      " [9.99131024e-01 7.54233566e-04 1.14794137e-04]\n",
      " [9.98883665e-01 1.01221260e-03 1.04065555e-04]\n",
      " [9.99360740e-01 5.55121980e-04 8.40633147e-05]\n",
      " [9.98638093e-01 1.27084565e-03 9.10821982e-05]\n",
      " [7.65804946e-02 9.21491504e-01 1.92802097e-03]\n",
      " [9.99466479e-01 4.49399289e-04 8.41339715e-05]\n",
      " [9.99459803e-01 4.11835645e-04 1.28292784e-04]\n",
      " [9.99111116e-01 7.20945012e-04 1.67922903e-04]\n",
      " [1.51862623e-03 9.95577216e-01 2.90417438e-03]\n",
      " [1.67086767e-03 9.91879106e-01 6.45004911e-03]\n",
      " [9.44700383e-04 9.98299420e-01 7.55897549e-04]\n",
      " [2.92506302e-04 3.63230228e-01 6.36477292e-01]\n",
      " [3.79133760e-03 9.47947919e-01 4.82608192e-02]\n",
      " [1.94703217e-03 9.93757129e-01 4.29587485e-03]\n",
      " [9.80457799e-07 2.42268608e-04 9.99756753e-01]\n",
      " [7.47417334e-06 2.75515555e-03 9.97237325e-01]\n",
      " [5.33800412e-05 2.08180416e-02 9.79128599e-01]\n",
      " [2.09249745e-04 7.46041089e-02 9.25186634e-01]\n",
      " [7.86635326e-04 2.56203145e-01 7.43010223e-01]\n",
      " [3.30222887e-04 2.34999090e-01 7.64670670e-01]\n",
      " [9.95201349e-01 3.82833020e-03 9.70315305e-04]\n",
      " [9.74264681e-01 2.31890120e-02 2.54626386e-03]\n",
      " [9.94026303e-01 4.26270626e-03 1.71093899e-03]\n",
      " [9.94783819e-01 4.64619603e-03 5.70051838e-04]\n",
      " [9.70916390e-01 2.75594220e-02 1.52417494e-03]\n",
      " [9.99014735e-01 6.55867800e-04 3.29383678e-04]\n",
      " [1.72258914e-02 9.69105244e-01 1.36688408e-02]\n",
      " [9.50826705e-03 9.89399076e-01 1.09260529e-03]\n",
      " [3.92032554e-04 7.64594555e-01 2.35013396e-01]\n",
      " [7.86982477e-02 9.10245061e-01 1.10567352e-02]\n",
      " [1.62626442e-03 9.90682006e-01 7.69170141e-03]\n",
      " [1.08584518e-05 5.05653815e-03 9.94932592e-01]\n",
      " [3.75836971e-05 6.98162941e-03 9.92980719e-01]\n",
      " [6.24019833e-07 5.19939756e-04 9.99479473e-01]\n",
      " [2.56014664e-05 1.93387605e-02 9.80635583e-01]\n",
      " [4.32737153e-07 1.38760384e-04 9.99860764e-01]\n",
      " [3.59254568e-06 1.50568865e-03 9.98490691e-01]\n",
      " [1.28924921e-06 1.18484197e-03 9.98813868e-01]\n",
      " [1.48361278e-06 4.41293669e-04 9.99557197e-01]\n",
      " [9.98235226e-01 1.37610442e-03 3.88655521e-04]\n",
      " [9.97785568e-01 1.58748857e-03 6.26984052e-04]\n",
      " [9.96991277e-01 2.51644361e-03 4.92324121e-04]\n",
      " [9.89063680e-01 8.72138701e-03 2.21493887e-03]\n",
      " [9.90675151e-01 7.83673115e-03 1.48817746e-03]\n",
      " [9.89839315e-01 9.64057073e-03 5.20103786e-04]\n",
      " [9.49533582e-01 4.90461998e-02 1.42027787e-03]\n",
      " [9.98828709e-01 9.63150407e-04 2.08074489e-04]\n",
      " [9.95573878e-01 3.43776075e-03 9.88398213e-04]\n",
      " [9.95848179e-01 3.84736876e-03 3.04441346e-04]\n",
      " [9.98720944e-01 8.65778187e-04 4.13205737e-04]\n",
      " [9.96633589e-01 2.29202746e-03 1.07443915e-03]\n",
      " [9.94791925e-01 4.70043300e-03 5.07556542e-04]\n",
      " [9.97231185e-01 1.72978570e-03 1.03898207e-03]\n",
      " [9.91975486e-01 6.80650817e-03 1.21795910e-03]\n",
      " [9.99331594e-01 5.36703039e-04 1.31777721e-04]\n",
      " [9.99389291e-01 4.00217599e-04 2.10542363e-04]\n",
      " [5.18550107e-04 9.94116306e-01 5.36503922e-03]\n",
      " [3.47533874e-04 9.06738698e-01 9.29138064e-02]\n",
      " [1.76427056e-04 9.19640124e-01 8.01834464e-02]\n",
      " [3.23922909e-03 9.92992878e-01 3.76799051e-03]\n",
      " [1.77709706e-04 5.43705404e-01 4.56116885e-01]\n",
      " [2.88769690e-04 8.26287210e-01 1.73424006e-01]\n",
      " [2.08094003e-04 3.54949892e-01 6.44842029e-01]\n",
      " [6.44040992e-04 9.91414487e-01 7.94146489e-03]\n",
      " [3.74552299e-04 9.53960955e-01 4.56644110e-02]\n",
      " [9.02442320e-04 9.95667577e-01 3.43003287e-03]\n",
      " [1.50561018e-05 3.48805566e-03 9.96496856e-01]\n",
      " [1.64650953e-06 5.94600744e-04 9.99403715e-01]\n",
      " [5.42506223e-06 1.97539106e-03 9.98019218e-01]\n",
      " [1.13117210e-06 2.30769016e-04 9.99768078e-01]\n",
      " [3.00715953e-07 1.79359733e-04 9.99820411e-01]\n",
      " [2.91876461e-07 9.80317636e-05 9.99901652e-01]\n",
      " [1.27315150e-06 1.79489551e-04 9.99819219e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fRes.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
