{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, RNN, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Data Loading, normalization, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reshape_to_input(X, T, Mat_dim):\n",
    "    \"\"\"\n",
    "    reshapes data\n",
    "    \"\"\"\n",
    "   \n",
    "    [Num_examples, dim] = X.shape\n",
    "    input_X = X[:,2:].reshape(Num_examples, Mat_dim, T)\n",
    "    \n",
    "    return input_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_towards_mean_train(X):\n",
    "   # print(X.shape)\n",
    "    \"# Preprocessing: Subtract the mean feature - a question if it is feasible - all features are already in the same scale...\"\n",
    "    \" returns the mean and standard deviation from the train set\"\n",
    "    # TODO: try -1 1 normalization\n",
    "    mean_feat = np.mean(X, axis=1, keepdims=True)\n",
    "   # print(mean_feat.shape)\n",
    "    X -= mean_feat\n",
    "    # Preprocessing: Divide by standard deviation. This ensures that each feature\\n\",\n",
    "    # has roughly the same scale.\\n\",\n",
    "    std_feat = np.std(X, axis=0, keepdims=True)\n",
    "    X /= std_feat\n",
    "    \n",
    "    return X, mean_feat, std_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_towards_mean_testval(X, mean_feat, std_feat):\n",
    "   # print(X.shape)\n",
    "    \"# normalize test & val set...\\n\",\n",
    "    # TODO: try -1 1 normalization\n",
    "    X -= mean_feat\n",
    "    # Preprocessing: Divide by standard deviation. This ensures that each feature\\n\",\n",
    "    # has roughly the same scale.\\n\",\n",
    "    X /= std_feat\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(inputY, C):\n",
    "    \"Convert Y to one hot representation\"\n",
    "    N= inputY.size\n",
    "    Y=np.zeros((N,C))\n",
    "    for i in range (0, inputY.size):\n",
    "        Y[i, int(inputY[i]-1)] = 1\n",
    "        \n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compund_model_assess(matrix, val_acc):\n",
    "    \" take the output of models and accuracy, and get the final score\"\n",
    "    num_classes = 3\n",
    "    ex, dim = matrice_models_for_test.shape\n",
    "    num_models = dim/3\n",
    "    \n",
    "    #binary assessment\n",
    "    # TO DO\n",
    "    \n",
    "    matrice_final =  np.zeros((ex,num_classes))\n",
    "    for i in range(0,dim,3):\n",
    "        matrice_final += matrix[:,i:i+3] * val_acc[int(i/3)]\n",
    "        # weight the model according the validation accuracy of the model\n",
    "    matrice_final /= num_models\n",
    "    return matrice_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Gait model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    units = 64 \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "\n",
    "    X = Bidirectional(LSTM(units, return_sequences=True))( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "   \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    X = Bidirectional(LSTM(units, return_sequences=False),merge_mode='concat')(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my dataset and parameters from a file\n",
    "from data_utils_modified import loadfromfolder,Random_Selection_train_test, Selection_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myoptim=Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(filepath, save_best_only =True, monitor = 'val_loss', mode ='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min') #epsilon\n",
    "    return [es, mcp_save, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covmat_cycles_frames_12t_36f_angles_22persons.csv\n",
      "Persons for train: \n",
      "[ 2.  3.  4.  5.  6.  7.  8. 10. 11. 12. 13. 14. 16. 17. 19. 22.]\n",
      "Persons for test: \n",
      "[ 1  9 15 18 20 21]\n",
      "Persons for train: \n",
      "[ 2.  3.  4.  5.  8. 12. 13. 14. 16. 17. 19.]\n",
      "Persons for test: \n",
      "[ 6.  7. 10. 11. 22.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khokhlov/virtenv/ml36/lib/python3.5/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 264 samples, validate on 134 samples\n",
      "Epoch 1/40\n",
      "264/264 [==============================] - 11s 42ms/step - loss: 1.1300 - acc: 0.3485 - val_loss: 1.0448 - val_acc: 0.5373\n",
      "Epoch 2/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 1.0212 - acc: 0.5189 - val_loss: 0.9730 - val_acc: 0.7687\n",
      "Epoch 3/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.9714 - acc: 0.5114 - val_loss: 0.9061 - val_acc: 0.7910\n",
      "Epoch 4/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.9146 - acc: 0.6364 - val_loss: 0.8361 - val_acc: 0.7761\n",
      "Epoch 5/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.8190 - acc: 0.7197 - val_loss: 0.7557 - val_acc: 0.7836\n",
      "Epoch 6/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.7657 - acc: 0.7045 - val_loss: 0.6695 - val_acc: 0.7985\n",
      "Epoch 7/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.6888 - acc: 0.7462 - val_loss: 0.5934 - val_acc: 0.7985\n",
      "Epoch 8/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.6379 - acc: 0.7500 - val_loss: 0.5283 - val_acc: 0.8060\n",
      "Epoch 9/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.5648 - acc: 0.7992 - val_loss: 0.4879 - val_acc: 0.7985\n",
      "Epoch 10/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.5334 - acc: 0.7841 - val_loss: 0.4460 - val_acc: 0.7985\n",
      "Epoch 11/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.4839 - acc: 0.8144 - val_loss: 0.4331 - val_acc: 0.8209\n",
      "Epoch 12/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.4154 - acc: 0.8485 - val_loss: 0.3960 - val_acc: 0.8284\n",
      "Epoch 13/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.3956 - acc: 0.8409 - val_loss: 0.3875 - val_acc: 0.8582\n",
      "Epoch 14/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.3541 - acc: 0.8674 - val_loss: 0.4070 - val_acc: 0.8507\n",
      "Epoch 15/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.3135 - acc: 0.9053 - val_loss: 0.3775 - val_acc: 0.8657\n",
      "Epoch 16/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.2774 - acc: 0.9091 - val_loss: 0.3514 - val_acc: 0.8284\n",
      "Epoch 17/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.2606 - acc: 0.9205 - val_loss: 0.3459 - val_acc: 0.8433\n",
      "Epoch 18/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.2550 - acc: 0.9015 - val_loss: 0.3617 - val_acc: 0.8358\n",
      "Epoch 19/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.2047 - acc: 0.9242 - val_loss: 0.3720 - val_acc: 0.8358\n",
      "Epoch 20/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.1921 - acc: 0.9280 - val_loss: 0.3840 - val_acc: 0.8582\n",
      "Epoch 21/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.1636 - acc: 0.9583 - val_loss: 0.4009 - val_acc: 0.8284\n",
      "Epoch 22/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.1554 - acc: 0.9583 - val_loss: 0.3991 - val_acc: 0.8433\n",
      "Epoch 23/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.1241 - acc: 0.9583 - val_loss: 0.3996 - val_acc: 0.8507\n",
      "Epoch 24/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.1254 - acc: 0.9659 - val_loss: 0.4016 - val_acc: 0.8507\n",
      "Epoch 25/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.1277 - acc: 0.9697 - val_loss: 0.4119 - val_acc: 0.8657\n",
      "Epoch 26/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.1296 - acc: 0.9432 - val_loss: 0.4719 - val_acc: 0.8433\n",
      "Epoch 27/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.1335 - acc: 0.9545 - val_loss: 0.3917 - val_acc: 0.8731\n",
      "Epoch 28/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.0940 - acc: 0.9811 - val_loss: 0.3899 - val_acc: 0.8731\n",
      "Epoch 29/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.0735 - acc: 0.9886 - val_loss: 0.4231 - val_acc: 0.8731\n",
      "Epoch 30/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.0818 - acc: 0.9773 - val_loss: 0.4476 - val_acc: 0.8806\n",
      "Epoch 31/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.0622 - acc: 0.9811 - val_loss: 0.4711 - val_acc: 0.8507\n",
      "Epoch 32/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.0601 - acc: 0.9811 - val_loss: 0.4510 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "149/149 [==============================] - 0s 3ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "0.7449664461532696 0.8432835820895522\n",
      "Persons for train: \n",
      "[ 2.  3.  4.  7.  8. 11. 13. 14. 16. 19. 22.]\n",
      "Persons for test: \n",
      "[ 5.  6. 10. 12. 17.]\n",
      "Train on 269 samples, validate on 129 samples\n",
      "Epoch 1/40\n",
      "269/269 [==============================] - 7s 26ms/step - loss: 1.0762 - acc: 0.4275 - val_loss: 1.0430 - val_acc: 0.5581\n",
      "Epoch 2/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 1.0673 - acc: 0.4498 - val_loss: 1.0147 - val_acc: 0.6279\n",
      "Epoch 3/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 1.0333 - acc: 0.4647 - val_loss: 0.9914 - val_acc: 0.6744\n",
      "Epoch 4/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 1.0145 - acc: 0.5353 - val_loss: 0.9713 - val_acc: 0.6977\n",
      "Epoch 5/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.9893 - acc: 0.5762 - val_loss: 0.9538 - val_acc: 0.6977\n",
      "Epoch 6/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.9574 - acc: 0.6208 - val_loss: 0.9374 - val_acc: 0.7054\n",
      "Epoch 7/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.9486 - acc: 0.6245 - val_loss: 0.9217 - val_acc: 0.7054\n",
      "Epoch 8/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.9430 - acc: 0.6431 - val_loss: 0.9064 - val_acc: 0.7054\n",
      "Epoch 9/40\n",
      "269/269 [==============================] - 4s 16ms/step - loss: 0.9202 - acc: 0.6803 - val_loss: 0.8926 - val_acc: 0.7054\n",
      "Epoch 10/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.9153 - acc: 0.6134 - val_loss: 0.8794 - val_acc: 0.7132\n",
      "Epoch 11/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.8806 - acc: 0.7138 - val_loss: 0.8663 - val_acc: 0.7132\n",
      "Epoch 12/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.8587 - acc: 0.7026 - val_loss: 0.8534 - val_acc: 0.7364\n",
      "Epoch 13/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.8753 - acc: 0.6580 - val_loss: 0.8406 - val_acc: 0.7209\n",
      "Epoch 14/40\n",
      "269/269 [==============================] - 4s 13ms/step - loss: 0.8583 - acc: 0.6914 - val_loss: 0.8283 - val_acc: 0.7364\n",
      "Epoch 15/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.8319 - acc: 0.6506 - val_loss: 0.8160 - val_acc: 0.7364\n",
      "Epoch 16/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.8518 - acc: 0.6357 - val_loss: 0.8043 - val_acc: 0.7364\n",
      "Epoch 17/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.8338 - acc: 0.6691 - val_loss: 0.7927 - val_acc: 0.7364\n",
      "Epoch 18/40\n",
      "269/269 [==============================] - 4s 13ms/step - loss: 0.8179 - acc: 0.6989 - val_loss: 0.7809 - val_acc: 0.7364\n",
      "Epoch 19/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.8167 - acc: 0.6729 - val_loss: 0.7694 - val_acc: 0.7442\n",
      "Epoch 20/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7840 - acc: 0.7212 - val_loss: 0.7582 - val_acc: 0.7442\n",
      "Epoch 21/40\n",
      "269/269 [==============================] - 4s 13ms/step - loss: 0.7797 - acc: 0.7063 - val_loss: 0.7467 - val_acc: 0.7519\n",
      "Epoch 22/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7632 - acc: 0.7175 - val_loss: 0.7355 - val_acc: 0.7519\n",
      "Epoch 23/40\n",
      "269/269 [==============================] - 4s 13ms/step - loss: 0.7610 - acc: 0.6877 - val_loss: 0.7246 - val_acc: 0.7519\n",
      "Epoch 24/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7328 - acc: 0.7509 - val_loss: 0.7138 - val_acc: 0.7519\n",
      "Epoch 25/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7257 - acc: 0.7286 - val_loss: 0.7030 - val_acc: 0.7519\n",
      "Epoch 26/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7160 - acc: 0.7398 - val_loss: 0.6926 - val_acc: 0.7674\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7011 - acc: 0.7249 - val_loss: 0.6829 - val_acc: 0.7597\n",
      "Epoch 28/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.7068 - acc: 0.7175 - val_loss: 0.6732 - val_acc: 0.7674\n",
      "Epoch 29/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.6892 - acc: 0.7770 - val_loss: 0.6632 - val_acc: 0.7752\n",
      "Epoch 30/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6979 - acc: 0.7249 - val_loss: 0.6535 - val_acc: 0.7829\n",
      "Epoch 31/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6577 - acc: 0.7658 - val_loss: 0.6443 - val_acc: 0.7829\n",
      "Epoch 32/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6707 - acc: 0.7584 - val_loss: 0.6354 - val_acc: 0.7752\n",
      "Epoch 33/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6578 - acc: 0.7621 - val_loss: 0.6270 - val_acc: 0.7674\n",
      "Epoch 34/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.6747 - acc: 0.7398 - val_loss: 0.6189 - val_acc: 0.7752\n",
      "Epoch 35/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6537 - acc: 0.7361 - val_loss: 0.6107 - val_acc: 0.7752\n",
      "Epoch 36/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6418 - acc: 0.7621 - val_loss: 0.6021 - val_acc: 0.7829\n",
      "Epoch 37/40\n",
      "269/269 [==============================] - 4s 14ms/step - loss: 0.6051 - acc: 0.7881 - val_loss: 0.5941 - val_acc: 0.7907\n",
      "Epoch 38/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.6179 - acc: 0.7658 - val_loss: 0.5866 - val_acc: 0.7907\n",
      "Epoch 39/40\n",
      "269/269 [==============================] - 4s 16ms/step - loss: 0.6115 - acc: 0.7546 - val_loss: 0.5793 - val_acc: 0.7907\n",
      "Epoch 40/40\n",
      "269/269 [==============================] - 4s 15ms/step - loss: 0.5857 - acc: 0.7807 - val_loss: 0.5725 - val_acc: 0.7984\n",
      "149/149 [==============================] - 0s 3ms/step\n",
      "129/129 [==============================] - 0s 4ms/step\n",
      "0.7181208081693458 0.7984496124031008\n",
      "Persons for train: \n",
      "[ 2.  3.  4.  6.  8. 12. 13. 14. 16. 17. 19.]\n",
      "Persons for test: \n",
      "[ 5.  7. 10. 11. 22.]\n",
      "Train on 264 samples, validate on 134 samples\n",
      "Epoch 1/40\n",
      "264/264 [==============================] - 8s 32ms/step - loss: 1.0884 - acc: 0.3902 - val_loss: 1.1141 - val_acc: 0.2985\n",
      "Epoch 2/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 1.0860 - acc: 0.3561 - val_loss: 1.0660 - val_acc: 0.3209\n",
      "Epoch 3/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 1.0442 - acc: 0.3977 - val_loss: 1.0242 - val_acc: 0.5149\n",
      "Epoch 4/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 1.0241 - acc: 0.4697 - val_loss: 0.9911 - val_acc: 0.5970\n",
      "Epoch 5/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.9999 - acc: 0.4848 - val_loss: 0.9619 - val_acc: 0.6791\n",
      "Epoch 6/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.9712 - acc: 0.5795 - val_loss: 0.9374 - val_acc: 0.6940\n",
      "Epoch 7/40\n",
      "264/264 [==============================] - 4s 16ms/step - loss: 0.9475 - acc: 0.5909 - val_loss: 0.9134 - val_acc: 0.7239\n",
      "Epoch 8/40\n",
      "264/264 [==============================] - 4s 17ms/step - loss: 0.9388 - acc: 0.5985 - val_loss: 0.8917 - val_acc: 0.7463\n",
      "Epoch 9/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.9259 - acc: 0.6629 - val_loss: 0.8695 - val_acc: 0.7537\n",
      "Epoch 10/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.9053 - acc: 0.6364 - val_loss: 0.8488 - val_acc: 0.7313\n",
      "Epoch 11/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.8843 - acc: 0.6742 - val_loss: 0.8281 - val_acc: 0.7537\n",
      "Epoch 12/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.8654 - acc: 0.6667 - val_loss: 0.8084 - val_acc: 0.7612\n",
      "Epoch 13/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.8544 - acc: 0.6705 - val_loss: 0.7883 - val_acc: 0.7612\n",
      "Epoch 14/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.8295 - acc: 0.7008 - val_loss: 0.7692 - val_acc: 0.7463\n",
      "Epoch 15/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.8385 - acc: 0.6705 - val_loss: 0.7500 - val_acc: 0.7463\n",
      "Epoch 16/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.8157 - acc: 0.6932 - val_loss: 0.7312 - val_acc: 0.7463\n",
      "Epoch 17/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.8040 - acc: 0.6894 - val_loss: 0.7127 - val_acc: 0.7612\n",
      "Epoch 18/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.8003 - acc: 0.7121 - val_loss: 0.6950 - val_acc: 0.7761\n",
      "Epoch 19/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7880 - acc: 0.6818 - val_loss: 0.6798 - val_acc: 0.7761\n",
      "Epoch 20/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7684 - acc: 0.7159 - val_loss: 0.6643 - val_acc: 0.7687\n",
      "Epoch 21/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7370 - acc: 0.7273 - val_loss: 0.6492 - val_acc: 0.7612\n",
      "Epoch 22/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7471 - acc: 0.6780 - val_loss: 0.6348 - val_acc: 0.7687\n",
      "Epoch 23/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7228 - acc: 0.7273 - val_loss: 0.6200 - val_acc: 0.7761\n",
      "Epoch 24/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7313 - acc: 0.7197 - val_loss: 0.6061 - val_acc: 0.7761\n",
      "Epoch 25/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6911 - acc: 0.7348 - val_loss: 0.5933 - val_acc: 0.7836\n",
      "Epoch 26/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7017 - acc: 0.7045 - val_loss: 0.5823 - val_acc: 0.7761\n",
      "Epoch 27/40\n",
      "264/264 [==============================] - 4s 15ms/step - loss: 0.6868 - acc: 0.7348 - val_loss: 0.5702 - val_acc: 0.7761\n",
      "Epoch 28/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.7013 - acc: 0.7083 - val_loss: 0.5595 - val_acc: 0.7761\n",
      "Epoch 29/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6683 - acc: 0.7500 - val_loss: 0.5498 - val_acc: 0.7761\n",
      "Epoch 30/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6527 - acc: 0.7538 - val_loss: 0.5390 - val_acc: 0.7761\n",
      "Epoch 31/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6545 - acc: 0.7348 - val_loss: 0.5285 - val_acc: 0.7836\n",
      "Epoch 32/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6410 - acc: 0.7500 - val_loss: 0.5192 - val_acc: 0.7836\n",
      "Epoch 33/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6244 - acc: 0.7273 - val_loss: 0.5112 - val_acc: 0.7910\n",
      "Epoch 34/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6281 - acc: 0.7652 - val_loss: 0.5025 - val_acc: 0.7910\n",
      "Epoch 35/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6089 - acc: 0.7614 - val_loss: 0.4980 - val_acc: 0.7910\n",
      "Epoch 36/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.5917 - acc: 0.7765 - val_loss: 0.4912 - val_acc: 0.7910\n",
      "Epoch 37/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.5973 - acc: 0.7386 - val_loss: 0.4849 - val_acc: 0.7985\n",
      "Epoch 38/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.6034 - acc: 0.7500 - val_loss: 0.4788 - val_acc: 0.8060\n",
      "Epoch 39/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.5860 - acc: 0.7917 - val_loss: 0.4751 - val_acc: 0.8134\n",
      "Epoch 40/40\n",
      "264/264 [==============================] - 4s 14ms/step - loss: 0.5782 - acc: 0.7462 - val_loss: 0.4709 - val_acc: 0.8134\n",
      "149/149 [==============================] - 0s 3ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "0.6711409435976272 0.8134328367105171\n",
      "Persons for train: \n",
      "[ 2.  3.  4.  7.  8. 11. 12. 13. 14. 17. 19.]\n",
      "Persons for test: \n",
      "[ 5.  6. 10. 16. 22.]\n",
      "Train on 274 samples, validate on 124 samples\n",
      "Epoch 1/40\n",
      "274/274 [==============================] - 8s 30ms/step - loss: 1.2089 - acc: 0.2044 - val_loss: 1.1313 - val_acc: 0.3065\n",
      "Epoch 2/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 1.1467 - acc: 0.2555 - val_loss: 1.0915 - val_acc: 0.3226\n",
      "Epoch 3/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 1.0874 - acc: 0.3577 - val_loss: 1.0645 - val_acc: 0.4032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 1.0540 - acc: 0.4088 - val_loss: 1.0385 - val_acc: 0.5323\n",
      "Epoch 5/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 1.0216 - acc: 0.4964 - val_loss: 1.0203 - val_acc: 0.5887\n",
      "Epoch 6/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 1.0018 - acc: 0.5182 - val_loss: 1.0006 - val_acc: 0.6532\n",
      "Epoch 7/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.9701 - acc: 0.5219 - val_loss: 0.9844 - val_acc: 0.6694\n",
      "Epoch 8/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.9488 - acc: 0.6277 - val_loss: 0.9719 - val_acc: 0.6774\n",
      "Epoch 9/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.9484 - acc: 0.5766 - val_loss: 0.9579 - val_acc: 0.7016\n",
      "Epoch 10/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.9324 - acc: 0.6131 - val_loss: 0.9443 - val_acc: 0.7016\n",
      "Epoch 11/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.9256 - acc: 0.6058 - val_loss: 0.9298 - val_acc: 0.6774\n",
      "Epoch 12/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8998 - acc: 0.6277 - val_loss: 0.9181 - val_acc: 0.6774\n",
      "Epoch 13/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8796 - acc: 0.6387 - val_loss: 0.9073 - val_acc: 0.6855\n",
      "Epoch 14/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8656 - acc: 0.6788 - val_loss: 0.8962 - val_acc: 0.6694\n",
      "Epoch 15/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8525 - acc: 0.6642 - val_loss: 0.8857 - val_acc: 0.6935\n",
      "Epoch 16/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8362 - acc: 0.6606 - val_loss: 0.8741 - val_acc: 0.6694\n",
      "Epoch 17/40\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 0.8222 - acc: 0.7482 - val_loss: 0.8625 - val_acc: 0.6774\n",
      "Epoch 18/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8305 - acc: 0.6715 - val_loss: 0.8521 - val_acc: 0.6694\n",
      "Epoch 19/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8011 - acc: 0.7445 - val_loss: 0.8422 - val_acc: 0.7016\n",
      "Epoch 20/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.8105 - acc: 0.6752 - val_loss: 0.8299 - val_acc: 0.7016\n",
      "Epoch 21/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7608 - acc: 0.7518 - val_loss: 0.8180 - val_acc: 0.7016\n",
      "Epoch 22/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7624 - acc: 0.7117 - val_loss: 0.8078 - val_acc: 0.7016\n",
      "Epoch 23/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7684 - acc: 0.7153 - val_loss: 0.7987 - val_acc: 0.7016\n",
      "Epoch 24/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7540 - acc: 0.7372 - val_loss: 0.7886 - val_acc: 0.7016\n",
      "Epoch 25/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7683 - acc: 0.6752 - val_loss: 0.7794 - val_acc: 0.7016\n",
      "Epoch 26/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7345 - acc: 0.7372 - val_loss: 0.7681 - val_acc: 0.7016\n",
      "Epoch 27/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7262 - acc: 0.7372 - val_loss: 0.7572 - val_acc: 0.7016\n",
      "Epoch 28/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7234 - acc: 0.7226 - val_loss: 0.7473 - val_acc: 0.7097\n",
      "Epoch 29/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.7025 - acc: 0.7336 - val_loss: 0.7371 - val_acc: 0.6935\n",
      "Epoch 30/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6807 - acc: 0.7372 - val_loss: 0.7283 - val_acc: 0.7016\n",
      "Epoch 31/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6931 - acc: 0.7299 - val_loss: 0.7190 - val_acc: 0.7016\n",
      "Epoch 32/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6846 - acc: 0.7664 - val_loss: 0.7095 - val_acc: 0.6935\n",
      "Epoch 33/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6568 - acc: 0.7701 - val_loss: 0.7005 - val_acc: 0.7097\n",
      "Epoch 34/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6630 - acc: 0.7737 - val_loss: 0.6918 - val_acc: 0.7097\n",
      "Epoch 35/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6535 - acc: 0.7664 - val_loss: 0.6843 - val_acc: 0.7097\n",
      "Epoch 36/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6428 - acc: 0.7445 - val_loss: 0.6760 - val_acc: 0.7097\n",
      "Epoch 37/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6492 - acc: 0.7445 - val_loss: 0.6678 - val_acc: 0.7016\n",
      "Epoch 38/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6069 - acc: 0.7628 - val_loss: 0.6601 - val_acc: 0.7177\n",
      "Epoch 39/40\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 0.6311 - acc: 0.7847 - val_loss: 0.6524 - val_acc: 0.7097\n",
      "Epoch 40/40\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 0.6141 - acc: 0.7810 - val_loss: 0.6458 - val_acc: 0.7177\n",
      "149/149 [==============================] - 0s 3ms/step\n",
      "124/124 [==============================] - 0s 3ms/step\n",
      "0.6577181244056497 0.717741935483871\n",
      "Persons for train: \n",
      "[ 3.  4.  5.  7. 10. 11. 12. 14. 17. 19. 22.]\n",
      "Persons for test: \n",
      "[ 2.  6.  8. 13. 16.]\n",
      "Train on 290 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "290/290 [==============================] - 9s 31ms/step - loss: 1.0769 - acc: 0.4103 - val_loss: 1.0594 - val_acc: 0.3796\n",
      "Epoch 2/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 1.0299 - acc: 0.5241 - val_loss: 1.0184 - val_acc: 0.5093\n",
      "Epoch 3/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 1.0035 - acc: 0.5448 - val_loss: 0.9860 - val_acc: 0.5741\n",
      "Epoch 4/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.9340 - acc: 0.6103 - val_loss: 0.9602 - val_acc: 0.6019\n",
      "Epoch 5/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.9042 - acc: 0.6621 - val_loss: 0.9343 - val_acc: 0.6019\n",
      "Epoch 6/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.8862 - acc: 0.6793 - val_loss: 0.9117 - val_acc: 0.5926\n",
      "Epoch 7/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.8489 - acc: 0.6966 - val_loss: 0.8921 - val_acc: 0.6111\n",
      "Epoch 8/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.8179 - acc: 0.7207 - val_loss: 0.8721 - val_acc: 0.6111\n",
      "Epoch 9/40\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.7904 - acc: 0.7483 - val_loss: 0.8523 - val_acc: 0.6111\n",
      "Epoch 10/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.7778 - acc: 0.7241 - val_loss: 0.8347 - val_acc: 0.6204\n",
      "Epoch 11/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.7396 - acc: 0.7862 - val_loss: 0.8162 - val_acc: 0.6204\n",
      "Epoch 12/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.7119 - acc: 0.7655 - val_loss: 0.8003 - val_acc: 0.6204\n",
      "Epoch 13/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.7095 - acc: 0.7414 - val_loss: 0.7841 - val_acc: 0.6296\n",
      "Epoch 14/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.6752 - acc: 0.7793 - val_loss: 0.7699 - val_acc: 0.6296\n",
      "Epoch 15/40\n",
      "290/290 [==============================] - 4s 12ms/step - loss: 0.6566 - acc: 0.7828 - val_loss: 0.7572 - val_acc: 0.6296\n",
      "Epoch 16/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.6624 - acc: 0.7690 - val_loss: 0.7457 - val_acc: 0.6296\n",
      "Epoch 17/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.6310 - acc: 0.7793 - val_loss: 0.7335 - val_acc: 0.6296\n",
      "Epoch 18/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.6199 - acc: 0.7759 - val_loss: 0.7239 - val_acc: 0.6296\n",
      "Epoch 19/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.5884 - acc: 0.7828 - val_loss: 0.7155 - val_acc: 0.6296\n",
      "Epoch 20/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.5802 - acc: 0.7759 - val_loss: 0.7084 - val_acc: 0.6296\n",
      "Epoch 21/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.5704 - acc: 0.7966 - val_loss: 0.7010 - val_acc: 0.6204\n",
      "Epoch 22/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.5446 - acc: 0.8345 - val_loss: 0.6934 - val_acc: 0.6204\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 4s 13ms/step - loss: 0.5356 - acc: 0.8276 - val_loss: 0.6915 - val_acc: 0.6111\n",
      "Epoch 24/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.5446 - acc: 0.7897 - val_loss: 0.6880 - val_acc: 0.6111\n",
      "Epoch 25/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.5055 - acc: 0.8345 - val_loss: 0.6857 - val_acc: 0.6111\n",
      "Epoch 26/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.5243 - acc: 0.8069 - val_loss: 0.6830 - val_acc: 0.6111\n",
      "Epoch 27/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4985 - acc: 0.8207 - val_loss: 0.6768 - val_acc: 0.6111\n",
      "Epoch 28/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.5157 - acc: 0.7897 - val_loss: 0.6721 - val_acc: 0.6111\n",
      "Epoch 29/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.5019 - acc: 0.8069 - val_loss: 0.6721 - val_acc: 0.6111\n",
      "Epoch 30/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.5015 - acc: 0.7690 - val_loss: 0.6685 - val_acc: 0.6111\n",
      "Epoch 31/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4786 - acc: 0.8241 - val_loss: 0.6690 - val_acc: 0.6111\n",
      "Epoch 32/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4769 - acc: 0.8103 - val_loss: 0.6693 - val_acc: 0.6111\n",
      "Epoch 33/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4495 - acc: 0.8379 - val_loss: 0.6689 - val_acc: 0.6204\n",
      "Epoch 34/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4663 - acc: 0.8379 - val_loss: 0.6704 - val_acc: 0.6389\n",
      "Epoch 35/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4545 - acc: 0.8586 - val_loss: 0.6708 - val_acc: 0.6389\n",
      "Epoch 36/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4489 - acc: 0.8448 - val_loss: 0.6677 - val_acc: 0.6389\n",
      "Epoch 37/40\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.4413 - acc: 0.8310 - val_loss: 0.6675 - val_acc: 0.6481\n",
      "Epoch 38/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4342 - acc: 0.8310 - val_loss: 0.6683 - val_acc: 0.6481\n",
      "Epoch 39/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4292 - acc: 0.8448 - val_loss: 0.6722 - val_acc: 0.6481\n",
      "Epoch 40/40\n",
      "290/290 [==============================] - 4s 13ms/step - loss: 0.4299 - acc: 0.8379 - val_loss: 0.6681 - val_acc: 0.6481\n",
      "149/149 [==============================] - 0s 3ms/step\n",
      "108/108 [==============================] - 0s 4ms/step\n",
      "0.6711409423975336 0.6481481459405687\n",
      "[0.74496645 0.71812081 0.67114094 0.65771812 0.67114094]\n",
      "Final Model accuracy from 5 compound models is 0.765101 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters needed\n",
    "trials = 1\n",
    "models = 5\n",
    "epochs = 40\n",
    "batch_size = 21\n",
    "\n",
    "units = 64 # actually defind inside the funciton of the model - TODO - make it here\n",
    "path ='data/best'\n",
    "Files, T, max_dim = loadfromfolder()\n",
    "#Where Files are names, T - my division into frames and max_dim - feature dimensionality\n",
    "fRes = open(\"Results_BiDirCompound_covmat.txt\",\"a+\") \n",
    "fRes.write(\"Number of units in LSTM = %d,  Epochs =  %d and batch size =  %d \\r\\n\" % (units, epochs,batch_size))\n",
    "\n",
    "for file in range(0,len(Files)): \n",
    "    name = Files[file]\n",
    "    t = T[file]\n",
    "    dim = max_dim[file]\n",
    "    name_w_path = path +\"/\"+name\n",
    "    fRes.write(\"Experiment %d from file %s \\r\\n\" % (file,name))\n",
    "    # here all the routine in One cell \n",
    "    X_all = pd.read_csv(name_w_path)\n",
    "    # the routine to run the same test N times, randomly shuffling the data\n",
    "    for j in range(0,trials):\n",
    "        fRes.write(\"Data partinioning random  %d out of %d\\r\\n\\n\" % (j, trials))\n",
    "        persons_testing = np.array([1, 9, 15, 18, 20, 21]) # specify which persons can be used for test\n",
    "        X_train_all, Y_train_all, X_test, Y_test = Selection_train_test(X_all.values, persons_testing)\n",
    "        Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "        X_test = Reshape_to_input(X_test, t, dim)\n",
    "    \n",
    "        matrice_models_for_test = np.array([], dtype=np.float).reshape(Y_test.size,0) \n",
    "        # here is the final matrix, which will save the probability assigned for each class\n",
    "        Accuracy  = np.zeros(shape=(models))\n",
    "        val_accuracy  = np.zeros(shape=(models))\n",
    "        for i in range(0,models):\n",
    "            \n",
    "            modelname ='f'+ str(file) +'t'+ str(j) + 'f' + str(i) #define model index\n",
    "        \n",
    "            X_train, Y_train, X_val, Y_val, returned_train = Random_Selection_train_test (X_train_all, 11)\n",
    "\n",
    "            Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "            Y_val_oh = convert_to_one_hot(Y_val, C = 3)\n",
    "        \n",
    "            X_train = Reshape_to_input(X_train, t, dim)\n",
    "            X_val = Reshape_to_input(X_val, t, dim)\n",
    "        \n",
    "        \n",
    "            model = Gait_model((X_train[0,:,:]))\n",
    "        \n",
    "            file_path = 'models/dif_vote/file_{}_model_wts.hdf5'.format(modelname)\n",
    "            callbacks = get_callbacks(filepath=file_path, patience = 35)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=myoptim, metrics=['accuracy'])\n",
    "              \n",
    "              \n",
    "        \n",
    "            model.fit(X_train, Y_train_oh, epochs=epochs,\\\n",
    "                      batch_size=batch_size, shuffle=True, \\\n",
    "                      validation_data=(X_val, Y_val_oh), callbacks=callbacks)\n",
    "        \n",
    "            loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "            loss, val_accuracy[i] = model.evaluate(X_val, Y_val_oh)\n",
    "            probability_assigned = model.predict(X_test) #verbose=1\n",
    "            matrice_models_for_test = np.hstack([matrice_models_for_test, probability_assigned]) # just stack each matrix horizontally\n",
    "            Accuracy[i]=acc\n",
    "            print(acc, val_accuracy[i])\n",
    "            fRes.write(\"Final accuracy per trial %f \\r\\n\" % (acc))\n",
    "        print(Accuracy)\n",
    "        fRes.write(\"Final mean accuracy is: %f \\r\\n\" % (np.mean(Accuracy)))\n",
    "        # calculate final compunt accuracy\n",
    "        final=compund_model_assess(matrice_models_for_test, val_accuracy)\n",
    "        Y_test_compound = np.argmax(final, axis=1)+1\n",
    "        print(\"Final Model accuracy from %d compound models is %f \\r\\n\\n\" % (models, accuracy_score(Y_test,Y_test_compound)))\n",
    "fRes.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different things to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(matrice_models_for_test, interpolation='nearest', cmap=plt.cm.ocean )\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "plt.pcolor(matrice_models_for_test)\n",
    "plt.colorbar()\n",
    "plt.axvline(x=[3], color='r', linestyle='-')\n",
    "plt.axvline(x=[6], color='r', linestyle='-')\n",
    "plt.axvline(x=[9], color='r', linestyle='-')\n",
    "plt.axvline(x=[12], color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model accuracy from 1 compound models is 0.483221\n"
     ]
    }
   ],
   "source": [
    "Y_test_compound = np.argmax(final, axis=1)+1\n",
    "print(\"Final Model accuracy from %d compound models is %f\" % (trials, accuracy_score(Y_test,Y_test_compound)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[39  5  9]\n",
      " [18  9 18]\n",
      " [22  5 24]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(Y_test,Y_test_compound)\n",
    "class_names = ['knee', 'normal', 'padding']\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 8, 15)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 8, 128)            40960     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_50 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 140,163\n",
      "Trainable params: 140,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = ([39,  1,  0],[ 10, 30,  2],[ 0,  1, 53]) + cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[132,  10,   0],\n",
       "       [ 14,  90,  35],\n",
       "       [  2,  10, 150]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M + cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.,  2.,  0.],\n",
       "       [ 5., 17.,  8.],\n",
       "       [ 0.,  1., 34.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(M/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = ([[39.,  2.,  0.],\n",
    "       [ 2., 30.,  8.],\n",
    "       [ 0.,  1., 34.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
