{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 -Gait Modeal:  LSTMs in Keras: \n",
    "\n",
    " LSTM model that takes as inputs my gait angles features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import tensorflow as tf\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486, 662)\n"
     ]
    }
   ],
   "source": [
    "# load data with Pandas\n",
    "import pandas as pd\n",
    "X_all = pd.read_csv(\"cycles_for_LSTM/X_all_10_66.csv\")\n",
    "# print things about the files\n",
    "print(X_all.shape)\n",
    "#print(X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Reshape_to_input(X):\n",
    "    \"\"\"\n",
    "    reshapes data\n",
    "    \"\"\"\n",
    "    max_length = 10\n",
    "    [Num_examples, dim] = X.shape\n",
    "    input_X = X[:,2:].reshape(Num_examples, max_length, 66)\n",
    "    \n",
    "    return input_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Random_Selection_train_test (X_all, N):\n",
    "#    import random\n",
    "    \" This fucntion takes return train and test samples for the LSTM: X dataset, N - number of training samples (i.e persons)\"\n",
    "    Persons = X_all[:,1]\n",
    "    uniquePersons = np.unique(Persons)\n",
    "    random.shuffle(uniquePersons)\n",
    "    num_dim = np.shape(X_all)[1]\n",
    "    X_train = np.array([], dtype=np.int64).reshape(0,num_dim)\n",
    "    Y_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    Y_test = np.array([])\n",
    "    print(X_train.shape)\n",
    "    for Person in range(1,N):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        #print(X_all[[indexes], :][0,0,:,:].shape)\n",
    "        Person_X_val = X_all[[indexes], :][0,0,:,:]\n",
    "        X_train= np.append(X_train,Person_X_val)\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_train = np.append(Y_train, X_all[[indexes], 0])\n",
    "    X_train=X_train.reshape(Y_train.size, num_dim)    \n",
    "    for Person in range(N,uniquePersons.size):\n",
    "        indexes = np.where(X_all[:,1]==uniquePersons[Person])\n",
    "        X_test = np.append(X_test, X_all[[indexes], :][0,0,:,:])\n",
    "        label =  np.full((indexes[0].size),uniquePersons[Person])\n",
    "        Y_test =  np.append(Y_test, X_all[[indexes], 0])\n",
    "    X_test=X_test.reshape(Y_test.size, num_dim)       \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(inputY, C):\n",
    "    N= inputY.size\n",
    "    Y=np.zeros((N,C))\n",
    "    for i in range (0, inputY.size):\n",
    "        Y[i, int(inputY[i]-1)] = 1\n",
    "        \n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 662)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = Random_Selection_train_test (X_all.values, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Keras and mini-batching \n",
    "\n",
    "In this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it's just not possible to do them both at the same time.\n",
    "\n",
    "The common solution to this is to use padding. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with \"0\"s so that each input sentence is of length 20. Thus, a sentence \"i love you\" would be represented as $(e_{i}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$. In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Gait_model(data):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input - data\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    gait_data = Input(data.shape, dtype='float32')\n",
    "    print(gait_data.shape)\n",
    "    #gait_data = tf.keras.backend.squeeze(gait_data, 0)\n",
    "    #examples = gait_data\n",
    "    #max_length = 15\n",
    "    #examples = examples[2:,:]\n",
    "    #print(examples.shape)\n",
    "    #Num_examples = examples.shape[0]\n",
    "    #embeddings = examples.reshape(Num_examples, max_length, 144)\n",
    "    #print(embeddings)\n",
    "    #embeddings = X_train# Reshape_to_input(X_train)\n",
    "    #embeddings = tf.convert_to_tensor(embeddings, name=\"raw_data\", dtype=tf.float32)\n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    print(gait_data.shape)\n",
    "    X = LSTM(128, return_sequences=True)( gait_data)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=gait_data, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-9930c323c9fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# prepare the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReshape_to_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReshape_to_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-7ce10ff06fd5>\u001b[0m in \u001b[0;36mReshape_to_input\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mNum_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0minput_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNum_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m66\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "X_train = Reshape_to_input(X_train)\n",
    "X_test = Reshape_to_input(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model and check its summary. Because all sentences in the dataset are less than 10 words, we chose `max_len = 10`.  You should see your architecture, it uses \"20,223,927\" parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001\\*50 = 20,000,050 non-trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267, 10, 66)\n",
      "(?, 10, 66)\n",
      "(?, 10, 66)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 10, 66)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 10, 128)           99840     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 231,811\n",
      "Trainable params: 231,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxLen = 10\n",
    "print(X_train.shape)\n",
    "model = Gait_model((X_train[0,:,:]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using `categorical_crossentropy` loss, `adam` optimizer and `['accuracy']` metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train your model. Your Emojifier-V2 `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 2/12\n",
      "262/262 [==============================] - 1s 5ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 3/12\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 4/12\n",
      "262/262 [==============================] - 1s 3ms/step - loss: 9.9687e-04 - acc: 1.0000\n",
      "Epoch 5/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 5.8223e-04 - acc: 1.0000\n",
      "Epoch 6/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 7.2041e-04 - acc: 1.0000\n",
      "Epoch 7/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 5.5999e-04 - acc: 1.0000\n",
      "Epoch 8/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 5.8533e-04 - acc: 1.0000\n",
      "Epoch 10/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 6.8329e-04 - acc: 1.0000\n",
      "Epoch 11/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 5.2282e-04 - acc: 1.0000\n",
      "Epoch 12/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 3.1852e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x60c9aac128>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train_oh, epochs = 12, batch_size = 15, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model should perform close to **100% accuracy** on the training set. The exact accuracy you get may be a little different. Run the following cell to evaluate your model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 0s 1ms/step\n",
      "\n",
      "Test accuracy =  0.95789473747\n"
     ]
    }
   ],
   "source": [
    "#X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "#Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.]\n",
      "(0, 662)\n",
      "Epoch 1/12\n",
      "265/265 [==============================] - 2s 6ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 2/12\n",
      "265/265 [==============================] - 1s 4ms/step - loss: 4.2743e-04 - acc: 1.0000\n",
      "Epoch 3/12\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 8.0727e-04 - acc: 1.0000\n",
      "Epoch 4/12\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 5.7747e-04 - acc: 1.0000\n",
      "Epoch 5/12\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 3.3361e-04 - acc: 1.0000\n",
      "Epoch 6/12\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 2.6743e-04 - acc: 1.0000\n",
      "Epoch 7/12\n",
      "265/265 [==============================] - 0s 2ms/step - loss: 2.0896e-04 - acc: 1.0000\n",
      "Epoch 8/12\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 1.7914e-04 - acc: 1.0000\n",
      "Epoch 9/12\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 2.0732e-04 - acc: 1.0000\n",
      "Epoch 10/12\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 1.5778e-04 - acc: 1.0000\n",
      "Epoch 11/12\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 1.8819e-04 - acc: 1.0000\n",
      "Epoch 12/12\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 2.1183e-04 - acc: 1.0000\n",
      "199/199 [==============================] - 0s 387us/step\n",
      "(0, 662)\n",
      "Epoch 1/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.9891\n",
      "Epoch 2/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0151 - acc: 0.9927\n",
      "Epoch 3/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.9891\n",
      "Epoch 4/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 5/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0073 - acc: 0.9964\n",
      "Epoch 6/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0109 - acc: 0.9964\n",
      "Epoch 7/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0064 - acc: 0.9964\n",
      "Epoch 8/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0101 - acc: 0.9964\n",
      "Epoch 9/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.9964\n",
      "Epoch 10/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0064 - acc: 0.9964\n",
      "Epoch 11/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.9964\n",
      "Epoch 12/12\n",
      "275/275 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.9927\n",
      "187/187 [==============================] - 0s 369us/step\n",
      "(0, 662)\n",
      "Epoch 1/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.1643 - acc: 0.9534\n",
      "Epoch 2/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0696 - acc: 0.9749\n",
      "Epoch 3/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0762 - acc: 0.9892\n",
      "Epoch 4/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 5/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 7/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 8.8172e-04 - acc: 1.0000\n",
      "Epoch 8/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 10/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 7.1035e-04 - acc: 1.0000\n",
      "Epoch 11/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 4.6119e-04 - acc: 1.0000\n",
      "Epoch 12/12\n",
      "279/279 [==============================] - 1s 2ms/step - loss: 4.8332e-04 - acc: 1.0000\n",
      "181/181 [==============================] - 0s 354us/step\n",
      "(0, 662)\n",
      "Epoch 1/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 0.0687 - acc: 0.9789\n",
      "Epoch 2/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 3/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 0.0111 - acc: 0.9965\n",
      "Epoch 4/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 5/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 7/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 7.9164e-04 - acc: 1.0000\n",
      "Epoch 8/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 5.0740e-04 - acc: 1.0000\n",
      "Epoch 9/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 3.9221e-04 - acc: 1.0000\n",
      "Epoch 10/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 4.6561e-04 - acc: 1.0000\n",
      "Epoch 11/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 6.0028e-04 - acc: 1.0000\n",
      "Epoch 12/12\n",
      "285/285 [==============================] - 1s 2ms/step - loss: 3.4407e-04 - acc: 1.0000\n",
      "181/181 [==============================] - 0s 370us/step\n",
      "(0, 662)\n",
      "Epoch 1/12\n",
      "262/262 [==============================] - 0s 2ms/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 2/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 3/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 4/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 5/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0210 - acc: 0.9962\n",
      "Epoch 6/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0090 - acc: 0.9962\n",
      "Epoch 7/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0174 - acc: 0.9924\n",
      "Epoch 8/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.1815 - acc: 0.9618\n",
      "Epoch 9/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.1601 - acc: 0.9427\n",
      "Epoch 10/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0406 - acc: 0.9885\n",
      "Epoch 11/12\n",
      "262/262 [==============================] - 0s 2ms/step - loss: 0.0155 - acc: 0.9962\n",
      "Epoch 12/12\n",
      "262/262 [==============================] - 1s 2ms/step - loss: 0.0029 - acc: 1.0000\n",
      "190/190 [==============================] - 0s 348us/step\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "Accuracy = X_train = np.zeros(shape=(trials))\n",
    "\n",
    "for i in range(0,trials):\n",
    "    X_train, Y_train, X_test, Y_test = Random_Selection_train_test (X_all.values, 12)\n",
    "    Y_train_oh = convert_to_one_hot(Y_train, C = 3)\n",
    "    Y_test_oh = convert_to_one_hot(Y_test, C = 3)\n",
    "    X_train = Reshape_to_input(X_train)\n",
    "    X_test = Reshape_to_input(X_test)\n",
    "    model.fit(X_train, Y_train_oh, epochs = 12, batch_size = 15, shuffle=True)\n",
    "    loss, acc = model.evaluate(X_test, Y_test_oh)\n",
    "    Accuracy[i]=acc\n",
    "    \n",
    "print(Accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99497487  1.          0.97237569  0.99447514  0.9631579 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Accuracy)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
